<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation</title>
<!--Generated on Mon Jul 21 06:08:32 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Agentic Artificial Intelligence,  AI Ethics,  Inclusive Design,  Smart Home Automation,  Responsible Innovation
" lang="en" name="keywords"/>
<base href="/html/2507.15901v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S1" title="In Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S1.SS1" title="In I Introduction ‣ Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">I-A</span> </span><span class="ltx_text ltx_font_italic">The Evolving Landscape of Agentic AI and Smart Homes</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S1.SS2" title="In I Introduction ‣ Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">I-B</span> </span><span class="ltx_text ltx_font_italic">Addressing Vulnerable User Needs through Ethical AI Design</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S2" title="In Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Foundations of Agentic AI: Evolution, Frameworks, and Applications
</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S2.SS1" title="In II Foundations of Agentic AI: Evolution, Frameworks, and Applications ‣ Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Defining Agentic AI: From LLM-Agents to Autonomous Systems</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S2.SS2" title="In II Foundations of Agentic AI: Evolution, Frameworks, and Applications ‣ Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Key Agentic AI Frameworks and Architectures</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S2.SS3" title="In II Foundations of Agentic AI: Evolution, Frameworks, and Applications ‣ Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Current State of AI in Household Automation</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S3" title="In Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Responsible Innovation and AI Ethics: Guiding Principles for Development</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S3.SS1" title="In III Responsible Innovation and AI Ethics: Guiding Principles for Development ‣ Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Core Ethical Principles for AI</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S3.SS2" title="In III Responsible Innovation and AI Ethics: Guiding Principles for Development ‣ Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Responsible Innovation (RRI) and AI Governance Frameworks</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S4" title="In Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Human-Centered and Participatory Design in AI for Vulnerable Populations</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S4.SS1" title="In IV Human-Centered and Participatory Design in AI for Vulnerable Populations ‣ Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Principles of Human-Centered AI (HCAI)</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S4.SS2" title="In IV Human-Centered and Participatory Design in AI for Vulnerable Populations ‣ Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Participatory Design: Opportunities and Challenges for Inclusive AI</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S4.SS3" title="In IV Human-Centered and Participatory Design in AI for Vulnerable Populations ‣ Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Designing for Inclusivity: General Considerations for Diverse Users</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S5" title="In Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Ethical Alignment Features: Explainability, Consent, and Override Mechanisms</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S5.SS1" title="In V Ethical Alignment Features: Explainability, Consent, and Override Mechanisms ‣ Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">Explainable AI (XAI) for Agentic Systems: Techniques and Importance</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S5.SS2" title="In V Ethical Alignment Features: Explainability, Consent, and Override Mechanisms ‣ Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span> </span><span class="ltx_text ltx_font_italic">Designing for User Consent and Data Privacy in Proactive AI</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S5.SS3" title="In V Ethical Alignment Features: Explainability, Consent, and Override Mechanisms ‣ Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span> </span><span class="ltx_text ltx_font_italic">User Override and Control Mechanisms in Autonomous Systems</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S6" title="In Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Tailoring Agentic AI for Specific Vulnerable User Scenarios</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S6.SS1" title="In VI Tailoring Agentic AI for Specific Vulnerable User Scenarios ‣ Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-A</span> </span><span class="ltx_text ltx_font_italic">Agentic AI for Elderly Users: Support, Privacy, and Autonomy</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S6.SS2" title="In VI Tailoring Agentic AI for Specific Vulnerable User Scenarios ‣ Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-B</span> </span><span class="ltx_text ltx_font_italic">Agentic AI for Children: Child-Centered Design and Protection</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S6.SS3" title="In VI Tailoring Agentic AI for Specific Vulnerable User Scenarios ‣ Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-C</span> </span><span class="ltx_text ltx_font_italic">Agentic AI for Neurodivergent Users: Accessibility and Empowerment</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S7" title="In Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Data-Driven Insights for Ethical AI Design: Leveraging Social Media Analysis</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S7.SS1" title="In VII Data-Driven Insights for Ethical AI Design: Leveraging Social Media Analysis ‣ Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VII-A</span> </span><span class="ltx_text ltx_font_italic">Methodologies for Extracting User Needs and Ethical Concerns from Social Data</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S7.SS2" title="In VII Data-Driven Insights for Ethical AI Design: Leveraging Social Media Analysis ‣ Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VII-B</span> </span><span class="ltx_text ltx_font_italic">Application of NLP Tools in Ethical AI Research</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S8" title="In Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VIII </span><span class="ltx_text ltx_font_smallcaps">Incorporate Case Studies</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S8.SS1" title="In VIII Incorporate Case Studies ‣ Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VIII-A</span> </span><span class="ltx_text ltx_font_italic">Introduction to Case Studies</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S8.SS2" title="In VIII Incorporate Case Studies ‣ Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VIII-B</span> </span><span class="ltx_text ltx_font_italic">Deepening Analysis Through Participatory Design and Multi-Agent Dynamics</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S9" title="In Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IX </span><span class="ltx_text ltx_font_smallcaps">Recommendations for Ethical and Inclusive Design of Agentic AI in Household Automation</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S9.SS1" title="In IX Recommendations for Ethical and Inclusive Design of Agentic AI in Household Automation ‣ Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IX-A</span> </span><span class="ltx_text ltx_font_italic">Adopting an Ethical-by-Design Framework</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S9.SS2" title="In IX Recommendations for Ethical and Inclusive Design of Agentic AI in Household Automation ‣ Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IX-B</span> </span><span class="ltx_text ltx_font_italic">Implementing Inclusive Participatory Design</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S9.SS3" title="In IX Recommendations for Ethical and Inclusive Design of Agentic AI in Household Automation ‣ Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IX-C</span> </span><span class="ltx_text ltx_font_italic">Enhancing User Agency with Dynamic Consent and Control</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S9.SS4" title="In IX Recommendations for Ethical and Inclusive Design of Agentic AI in Household Automation ‣ Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IX-D</span> </span><span class="ltx_text ltx_font_italic">Mitigating Bias through a Socio-technical Ways</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S9.SS5" title="In IX Recommendations for Ethical and Inclusive Design of Agentic AI in Household Automation ‣ Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IX-E</span> </span><span class="ltx_text ltx_font_italic">Controlling Data-Driven Insights Ethically</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S9.SS6" title="In IX Recommendations for Ethical and Inclusive Design of Agentic AI in Household Automation ‣ Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IX-F</span> </span><span class="ltx_text ltx_font_italic">Simulating Multi-Agent Household Dynamics</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S10" title="In Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">X </span><span class="ltx_text ltx_font_smallcaps">Future Scope</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S11" title="In Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">XI </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Joydeep Chandra
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id1.1.id1">Department of CST</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id2.2.id2">Tsinghua University
<br class="ltx_break"/></span>Beijing, China 
<br class="ltx_break"/>joydeepc2002@gmail.com
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Satyam Kumar Navneet
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id3.1.id1">Department of CSE</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id4.2.id2">Chandigarh University
<br class="ltx_break"/></span>Mohali, India
<br class="ltx_break"/>navneetsatyamkumar@gmail.com
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id5.id1">The implementation of Artificial Intelligence (AI) in household environments, especially in the form of proactive autonomous agents, brings about possibilities of comfort and attention as well as it comes with intra or extramural ethical challenges. This article analyzes agentic AI and its applications, focusing on its move from reactive to proactive autonomy, privacy, fairness and user control. We review responsible innovation frameworks, human-centered design principles, and governance practices to distill practical guidance for ethical smart home systems. Vulnerable user groups such as elderly individuals, children, and neurodivergent who face higher risks of surveillance, bias, and privacy risks were studied in detail in context of Agentic AI. Design imperatives are highlighted such as tailored explainability, granular consent mechanisms, and robust override controls, supported by participatory and inclusive methodologies. It was also explored how data-driven insights, including social media analysis via Natural Language Processing(NLP), can inform specific user needs and ethical concerns. This survey aims to provide both a conceptual foundation and suggestions for developing transparent, inclusive, and trustworthy agentic AI in household automation.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Agentic Artificial Intelligence, AI Ethics, Inclusive Design, Smart Home Automation, Responsible Innovation

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<section class="ltx_subsection" id="S1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S1.SS1.4.1.1">I-A</span> </span><span class="ltx_text ltx_font_italic" id="S1.SS1.5.2">The Evolving Landscape of Agentic AI and Smart Homes</span>
</h3>
<div class="ltx_para" id="S1.SS1.p1">
<p class="ltx_p" id="S1.SS1.p1.1">Agentic AI has an ability to make decisions, act, and perform goal-oriented tasks with little human intervention, by perception of the environment, reasoning, and planning<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib44" title="">44</a>]</cite>. These capabilities have recenty improved through Large Language Models (LLMs)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib30" title="">30</a>]</cite> and Multimodal LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib53" title="">53</a>]</cite>, which can even act proactively and self-supervisingly, going far beyond traditional reactive AI way of doing things <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib54" title="">54</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.SS1.p2">
<p class="ltx_p" id="S1.SS1.p2.1">In smart homes, AI now powers functions such as lighting and climate automation, voice interaction, energy optimization, security monitoring, and predictive maintenance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib34" title="">34</a>]</cite>. Examples include the Google Nest Thermostat and Ring cameras, which leverage adaptive learning and facial recognition. This shift from user-commanded to proactive AI fundamentally redefines human–AI interaction, requiring mechanisms for transparency, consent, and override to maintain trust and accountability <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib57" title="">57</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib56" title="">56</a>]</cite>. As seen in Table <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S2.T1" title="TABLE I ‣ II-C Current State of AI in Household Automation ‣ II Foundations of Agentic AI: Evolution, Frameworks, and Applications ‣ Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_tag">I</span></a> Smart Home AI Solutions and Ethical Challenges can be seen with its mitigation strategies as well.</p>
</div>
<div class="ltx_para" id="S1.SS1.p3">
<p class="ltx_p" id="S1.SS1.p3.1">While proactive AI promises seamless living, it introduces ethical challenges: privacy risks, ubiquitous surveillance, and erosion of autonomy. Personalization often relies on extensive and opaque data collection, exemplified by policies like Amazon Echo transmitting voice data for analysis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib58" title="">58</a>]</cite>. Addressing these tensions necessitates embedding explainability, dynamic consent, and robust user control into agentic AI design <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib32" title="">32</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S1.SS2.4.1.1">I-B</span> </span><span class="ltx_text ltx_font_italic" id="S1.SS2.5.2">Addressing Vulnerable User Needs through Ethical AI Design</span>
</h3>
<div class="ltx_para" id="S1.SS2.p1">
<p class="ltx_p" id="S1.SS2.p1.1">The study focuses on vulnerable user scenarios such as the elderly, children, and neurodivergent individuals. After reviewing, the suggestions of unique set of ethical and design imperatives are introduced. Human-Centered AI (HCAI) is used to maximize the benefits of AI while rigorously minimizing its potential harms by considering its impact on end-users, service providers, broader stakeholders, and society at large <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib56" title="">56</a>]</cite>.In HCAI, the focus of Machine Learning (ML) is to develop AI systems that are sensitive to human needs, actions and social situation. This in turn allows Human-Centered Design (HCD) experts, ethicists, social scientists, legal scholars, and the immediate participation of the end-users to contribute and to address biases in the development of AI systems and to make ai fair and safe to use <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib35" title="">35</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.SS2.p2">
<p class="ltx_p" id="S1.SS2.p2.1">Vulnerable user groups are having diverse levels of digital literacy, unique cognitive differences, and an increased risk to privacy issues or the adverse effects of algorithmic bias <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib7" title="">7</a>]</cite>. A significant concern is that many existing AI frameworks are predominantly built upon neurotypical assumptions, overlooking the varied communication styles, behavioral patterns, and processing needs characteristic of neurodivergent individuals <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib52" title="">52</a>]</cite>. HCAI offers a way of reducing systemic bias in AI systems, specifically as it relates to disadvantaged demographics of users. HCAI’s focus on fairness, bias and related issues <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib56" title="">56</a>]</cite> is deeply pertinent here. Pre-existing societal bias might be accentuated by AI systems among vulnderable populations, resulting in an unfair treatment or discrimination and bias <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib21" title="">21</a>]</cite>.The multidisciplinary approach of HCAI together with diverse stakeholder involvement including vulnerable users enables a detailed system for detecting and addressing biases throughout all AI development stages <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib38" title="">38</a>]</cite>. The training of AI systems on neurotypical behavior data results in poor understanding of neurodivergent needs which produces unhelpful proactive decisions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib52" title="">52</a>]</cite>. The design process gains both technical excellence and social responsibility and genuine inclusiveness through the HCAI approach <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib8" title="">8</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.SS2.p3">
<p class="ltx_p" id="S1.SS2.p3.1">There are several limitations in developing smart home systems and agentic AI, first, while ethical guidelines like IEEE Ethically Aligned Design (EAD)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib31" title="">31</a>]</cite> and the EU AI Act outline broad principles<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib17" title="">17</a>]</cite>, there aren’t practical design patterns for applying these principles in real-world household AI systems. Second, existing studies emphasizes technical optimization and convenience, often ignoring inclusive design and participatory methods for vulnerable users such as the elderly, children, and neurodivergent individuals <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib7" title="">7</a>]</cite>. Third, studies on explainability and consent mechanisms are mostly theoretical, with few concrete frameworks designed for proactive, autonomous agents <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib57" title="">57</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib46" title="">46</a>]</cite>. Finally, the link between data-driven social insights and ethical design is not explored enough, leaving a gap in using large-scale user sentiment and concerns to guide responsible innovation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib58" title="">58</a>]</cite>. This review tackles these issues by combining perspectives from different fields and offering practical design guidelines for ethically aligned agentic AI in household settings.</p>
</div>
<div class="ltx_para" id="S1.SS2.p4">
<p class="ltx_p" id="S1.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S1.SS2.p4.1.1">Our contributions are summarized as follows</span>:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">Survey of Agentic AI in Household Automation:</span> This study reviews the evolution of agentic AI, current frameworks and ethical principles, contextualizing them for proactive household systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib18" title="">18</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">Analysis of Ethical Gaps and Vulnerable User Needs:</span> This study identifies ethical risks for elderly, children, and neurodivergent populations. For addressing these users, actions are suggested to tailored design imperatives such as adaptive explainability, granular consent, and override controls <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib7" title="">7</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">Integration of Human-Centered and Participatory Design with AI Ethics:</span> This from Human-Centered AI (HCAI) and Participatory Design to propose inclusive design strategies for agentic AI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib38" title="">38</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i4.p1.1.1">Technical Design Patterns for Ethical Alignment:</span> In this survey patternssuch as HITL<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib29" title="">29</a>]</cite> checkpoints, contextual consent flows, and transparency mechanismsthat operationalize abstract ethical principles in practical system architecture are studied <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib46" title="">46</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i5.p1">
<p class="ltx_p" id="S1.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i5.p1.1.1">Data-Driven Methodology for Ethical Insight Extraction:</span> This survey highlights the role of Natural Language Processing (NLP) in analyzing social media data to capture emerging user concerns, biases, and expectations for future smart home AI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib58" title="">58</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib27" title="">27</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i6.p1">
<p class="ltx_p" id="S1.I1.i6.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i6.p1.1.1">Research Roadmap for Responsible Innovation:</span> This survey outlines open challenges and future directions, emphasizing proactive governance, bias mitigation beyond technical fixes, and scalable participatory approaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib38" title="">38</a>]</cite>.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Foundations of Agentic AI: Evolution, Frameworks, and Applications
</span>
</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.4.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.5.2">Defining Agentic AI: From LLM-Agents to Autonomous Systems</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The concept of AI agents has evolved over time. Nowadays, it encompasses systems that are able to perceive their environment, reason out complex issues, and act to achieve certain goals <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib44" title="">44</a>]</cite>. This development has been largely influenced by the rapid advancement in generative AI, particularly with the emergence of Large Language Models (LLMs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib30" title="">30</a>]</cite> and Multimodal Large Language Models (MLLMs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib53" title="">53</a>]</cite>. These models have greatly expanded the capabilities of AI agents, including their ability to understand meaning, reason more deeply, and make decisions on their own <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib51" title="">51</a>]</cite>. A more specialized type of AI known as agentic AI is described as adaptable and able to autonomously define and pursue objectives, especially in complex and dynamic environments <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib54" title="">54</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Unlike traditional AI, agentic AI acts independently. Agentic AI is designed to work toward goals on its own without constant assistance from humans, in contrast to older systems that typically wait for explicit instructions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib30" title="">30</a>]</cite>. This implies that an agentic system is capable of producing internal processes similar to human thought, creating Thoughts, Reasoning, Plan, and Criticism for every step in an action sequence. Such self-monologuing capacity combined with the capability to combine several external tools enables agentic AI to address high-level objectives without needing explicit step-by-step direction from a human being <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib54" title="">54</a>]</cite>. For example, in scientific discovery, agentic AI systems are already revolutionizing research by automating the literature review, hypothesis generation, experimentation, and analysis of results, hence speed up the pace of scientific progress <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib6" title="">6</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">The varying levels of autonomy in agentic AI systems carry significant ethical implications. The literature defines agentic AI as capable of autonomous, goal-directed action <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib54" title="">54</a>]</cite>, existing on a continuum from highly supervised systems to those operating with considerable independence. The degree of autonomy directly influences the need for robust ethical controls. When an AI agent operates with minimal or no human oversight <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib12" title="">12</a>]</cite>, the risks of errors, unintended actions, or misalignments with human values increase substantially. In such cases, accountability mechanisms, explainability, and human override capabilities become critical <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib32" title="">32</a>]</cite>. This formulation of autonomy along the spectrum guides recommendations for establishing control points for user override and transparency in AI design. A delicate balance between the productivity benefits of AI autonomy and the necessity of maintaining significant human control and supervision is essential <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib21" title="">21</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.4.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.5.2">Key Agentic AI Frameworks and Architectures</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The emerging domain of agentic AI is facilitated by a number of pioneering frameworks and architectural styles that allow for complex autonomous behaviors. Auto-GPT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib54" title="">54</a>]</cite> is a leading example of an autonomous agent utilizing the advanced capabilities of Large Language Models (LLMs)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib30" title="">30</a>]</cite> for sophisticated decision-making tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib54" title="">54</a>]</cite>. The ability of Auto-GPT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib54" title="">54</a>]</cite> template-based agents to have self-monologue, in which the agent generates internal Thoughts, Reasoning, Plan, and Criticism for each action sequence step, is one of their distinguishing characteristics. These agents can pursue goals at higher levels without explicit, step-by-step human guidance thanks to their internal thought process, ability to combine multiple tools, and long-term memory retention <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib26" title="">26</a>]</cite>. Benchmarking studies have shown that higher-end LLMs, like GPT-4, perform better in Auto-GPT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib54" title="">54</a>]</cite> type-like decision-making environments. In addition, algorithms such as the Additional Opinions algorithm can greatly improve an agent’s performance by adding in supervised learners without resorting to large-scale fine-tuning of the base LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib32" title="">32</a>]</cite>. Even with these developments, Auto-GPT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib54" title="">54</a>]</cite> agents are still limited, such as by their early stages of ability to operate in real-world environments, deterministic control over actions, and contextuality of some performance-augmenting algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib21" title="">21</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">LangChain<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib27" title="">27</a>]</cite> is another foundational framework in the construction of agentic AI as an orchestration layer that applies leading-edge Natural Language Processing (NLP) research to support LLM-based reasoning and tool integration <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib27" title="">27</a>]</cite>. The framework leverages a large corpus of research work that investigates sophisticated reasoning methods for LLMs. These consist of Self-Discover, which allows LLMs to independently generate reasoning templates; Take a Step Back, a method for inducing reasoning by abstraction; and Plan-and-Solve Prompting, which decomposes difficult tasks into manageable subtasks that can be executed sequentially <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib47" title="">47</a>]</cite>. For tool orchestration, LangChain<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib27" title="">27</a>]</cite> combines ideas from architectures like HuggingGPT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib41" title="">41</a>]</cite>, with which LLMs can link and leverage disparate AI models from different machine learning communities, and CAMEL, which supports autonomous collaboration among communicative agents <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib26" title="">26</a>]</cite>. Such architectural schemes enable LLMs to not only comprehend and produce human language but also to plan, engage with external environments, and carry out multi-step tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib27" title="">27</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">The advanced reasoning structures in Auto-GPT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib54" title="">54</a>]</cite> and LangChain<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib27" title="">27</a>]</cite>-based agents, such as self-monologue, enhanced planning functionality, and stable tool integration, support more sophisticated autonomous behavior. These capabilities, however, inherently pose challenges for ethical development. Hallucination (production of factually incorrect data), brittleness (unpredictable breakdown under new circumstances), emergent behavior (unintended actions), and coordination failure (issues in multi-agent systems) are major issues <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib51" title="">51</a>]</cite>. Methods such as the Additional Opinions algorithm in Auto-GPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib54" title="">54</a>]</cite> and the ReAct framework <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib27" title="">27</a>]</cite> are explicit efforts to counter these vulnerabilities by feeding in external knowledge or human feedback into the agent’s reasoning loop. In a household automation agent, the safety and trustworthiness of these reasoning processes are of critical concern, particularly as interactions with vulnerable users are involved.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.4.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.5.2">Current State of AI in Household Automation</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">These days, artificial intelligence is essential to intelligent homes because it provides a variety of features that simplify and enhance daily life. Unbelievably, these AI systems can control security systems, turn on and off lights, adjust room temperature, and assist with home appliance maintenance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib34" title="">34</a>]</cite>. Smart homes are controlled mostly by voice assistants such as Amazon Alexa, Google Assistant, and Apple Siri. These assistants can receive natural language instructions and perform routines to manage various devices <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib55" title="">55</a>]</cite>. Security cameras based on AI, including Ring and Nest Cam, utilize technologies such as face recognition and motion detection to identify potential threats in real time. Smart home appliances also employ AI to conserve energy and alert users when maintenance is necessary <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib54" title="">54</a>]</cite>. For instance, the Moen Flo Smart Water Monitor employs AI to monitor water consumption and pinpoint leaks early, preventing costly damage <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib46" title="">46</a>]</cite>. In the future, smart homes will be even more interconnected, provide more tailored experiences, and boast AI assistants that will anticipate what users require, oftentimes before they are asked <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib31" title="">31</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">While the undeniable advantages and ease of AI-powered smart home technologies cannot be disputed, their mass use has at the same time raised tremendous concerns about ethics. These concerns are mainly focused on ubiquitous risks to users’ privacy as well as data security, mainly due to the intense collection of individuals’ information by ever-on microphones, cameras, and video surveillance systems, accompanied by possible reuse of the sensitive data by developers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib46" title="">46</a>]</cite>. In addition, bias and discrimination are a salient issue in these systems. The source of such biases could be in the parameters, weights of the model, or training data of the machine learning algorithms itself, which could result in discriminatory or unfair decision-making based on age, sex, wealth, health, race, or religion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib21" title="">21</a>]</cite>. User trust is often undermined by a lack of clarity over how data is stored, processed, and transferred, as well as restricted user control over their own personal information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib58" title="">58</a>]</cite>. The aggregate impact of these ethical issues is ubiquitous surveillance, greater inequality, and a blurring of human and non-human agency boundaries.</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">The vision of improved personalization and proactive AI assistants <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib34" title="">34</a>]</cite> relies on collecting a large pool of granular personal information, such as personally identifiable information (PII), behavior patterns, biometric data, location data, and communication patterns <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib58" title="">58</a>]</cite>. This creates an inherent conflict: to deliver genuinely proactive and tailored home automation, capable of providing timely reminders, appropriate suggestions, and independent decisions, AI must deeply understand user behaviors, routines, and even emotional states <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib24" title="">24</a>]</cite>. However, such extensive data collection, particularly in the private setting of a home, significantly diminishes privacy and heightens concerns about ubiquitous surveillance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib46" title="">46</a>]</cite>. The ethical challenge lies not only in whether data is gathered but in how much is collected, how it is used, and who retains control over it. The literature’s emphasis on consent prompts and override mechanisms aims to provide user agency in data-rich environments where AI takes proactive measures <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib32" title="">32</a>]</cite>. As users often struggle to understand how their data is handled or how decisions are made, the black box nature of many AI systems exacerbates these privacy concerns <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib45" title="">45</a>]</cite>.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Comparison of Smart Home AI Solutions and Ethical Challenges</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.1.1.1">
<span class="ltx_p" id="S2.T1.1.1.1.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.1.1.1.1" style="font-size:70%;">Device</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.1.2.1">
<span class="ltx_p" id="S2.T1.1.1.1.2.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.2.1.1.1" style="font-size:70%;">Core Functionality</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.1.3.1">
<span class="ltx_p" id="S2.T1.1.1.1.3.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.3.1.1.1" style="font-size:70%;">Key AI Features</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.1.4.1">
<span class="ltx_p" id="S2.T1.1.1.1.4.1.1" style="width:142.3pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.4.1.1.1" style="font-size:70%;">Primary Ethical Concerns</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.1.5.1">
<span class="ltx_p" id="S2.T1.1.1.1.5.1.1" style="width:85.4pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.5.1.1.1" style="font-size:70%;">Mitigation/User Controls</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.2.1.1.1">
<span class="ltx_p" id="S2.T1.1.2.1.1.1.1" style="width:56.9pt;"><span class="ltx_text" id="S2.T1.1.2.1.1.1.1.1" style="font-size:70%;">Amazon Alexa</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.2.1.2.1">
<span class="ltx_p" id="S2.T1.1.2.1.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="S2.T1.1.2.1.2.1.1.1" style="font-size:70%;">Voice Assistant</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.1.2.1.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.2.1.3.1">
<span class="ltx_p" id="S2.T1.1.2.1.3.1.1" style="width:71.1pt;"><span class="ltx_text" id="S2.T1.1.2.1.3.1.1.1" style="font-size:70%;">NLU, Adaptive Learning</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.1.2.1.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.2.1.4.1">
<span class="ltx_p" id="S2.T1.1.2.1.4.1.1" style="width:142.3pt;"><span class="ltx_text" id="S2.T1.1.2.1.4.1.1.1" style="font-size:70%;">Voice recording, auto data processing, limited deletion control, third-party permissions</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.1.2.1.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.2.1.5.1">
<span class="ltx_p" id="S2.T1.1.2.1.5.1.1" style="width:85.4pt;"><span class="ltx_text" id="S2.T1.1.2.1.5.1.1.1" style="font-size:70%;">Opt-out for recordings, skill management, microphone mute</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.3.2.1.1">
<span class="ltx_p" id="S2.T1.1.3.2.1.1.1" style="width:56.9pt;"><span class="ltx_text" id="S2.T1.1.3.2.1.1.1.1" style="font-size:70%;">Google Nest</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.3.2.2.1">
<span class="ltx_p" id="S2.T1.1.3.2.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="S2.T1.1.3.2.2.1.1.1" style="font-size:70%;">Thermostat, Camera</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.3.2.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.3.2.3.1">
<span class="ltx_p" id="S2.T1.1.3.2.3.1.1" style="width:71.1pt;"><span class="ltx_text" id="S2.T1.1.3.2.3.1.1.1" style="font-size:70%;">Adaptive Learning, Facial Recognition</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.3.2.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.3.2.4.1">
<span class="ltx_p" id="S2.T1.1.3.2.4.1.1" style="width:142.3pt;"><span class="ltx_text" id="S2.T1.1.3.2.4.1.1.1" style="font-size:70%;">Data collection, video surveillance, facial bias, unclear data use </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.1.3.2.4.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib34" title="">34</a><span class="ltx_text" id="S2.T1.1.3.2.4.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.3.2.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.3.2.5.1">
<span class="ltx_p" id="S2.T1.1.3.2.5.1.1" style="width:85.4pt;"><span class="ltx_text" id="S2.T1.1.3.2.5.1.1.1" style="font-size:70%;">Audio opt-out, activity settings</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.4.3.1.1">
<span class="ltx_p" id="S2.T1.1.4.3.1.1.1" style="width:56.9pt;"><span class="ltx_text" id="S2.T1.1.4.3.1.1.1.1" style="font-size:70%;">Ring Video Doorbell</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.4.3.2.1">
<span class="ltx_p" id="S2.T1.1.4.3.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="S2.T1.1.4.3.2.1.1.1" style="font-size:70%;">Doorbell</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.4.3.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.4.3.3.1">
<span class="ltx_p" id="S2.T1.1.4.3.3.1.1" style="width:71.1pt;"><span class="ltx_text" id="S2.T1.1.4.3.3.1.1.1" style="font-size:70%;">Facial Recognition (emerging)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.4.3.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.4.3.4.1">
<span class="ltx_p" id="S2.T1.1.4.3.4.1.1" style="width:142.3pt;"><span class="ltx_text" id="S2.T1.1.4.3.4.1.1.1" style="font-size:70%;">Illicit filming, law enforcement data sharing, surveillance risks </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.1.4.3.4.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib55" title="">55</a><span class="ltx_text" id="S2.T1.1.4.3.4.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.4.3.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.4.3.5.1">
<span class="ltx_p" id="S2.T1.1.4.3.5.1.1" style="width:85.4pt;"><span class="ltx_text" id="S2.T1.1.4.3.5.1.1.1" style="font-size:70%;">Footage sharing consent, no facial recognition </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.1.4.3.5.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib55" title="">55</a><span class="ltx_text" id="S2.T1.1.4.3.5.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.5.4.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.5.4.1.1">
<span class="ltx_p" id="S2.T1.1.5.4.1.1.1" style="width:56.9pt;"><span class="ltx_text" id="S2.T1.1.5.4.1.1.1.1" style="font-size:70%;">Philips Hue</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.5.4.2.1">
<span class="ltx_p" id="S2.T1.1.5.4.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="S2.T1.1.5.4.2.1.1.1" style="font-size:70%;">Smart Lighting</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.5.4.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.5.4.3.1">
<span class="ltx_p" id="S2.T1.1.5.4.3.1.1" style="width:71.1pt;"><span class="ltx_text" id="S2.T1.1.5.4.3.1.1.1" style="font-size:70%;">Adaptive Lighting</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.5.4.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.5.4.4.1">
<span class="ltx_p" id="S2.T1.1.5.4.4.1.1" style="width:142.3pt;"><span class="ltx_text" id="S2.T1.1.5.4.4.1.1.1" style="font-size:70%;">Presence tracking, energy data exposure </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.1.5.4.4.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib34" title="">34</a><span class="ltx_text" id="S2.T1.1.5.4.4.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.5.4.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.5.4.5.1">
<span class="ltx_p" id="S2.T1.1.5.4.5.1.1" style="width:85.4pt;"><span class="ltx_text" id="S2.T1.1.5.4.5.1.1.1" style="font-size:70%;">Limited platform privacy controls </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.1.5.4.5.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib46" title="">46</a><span class="ltx_text" id="S2.T1.1.5.4.5.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.6.5.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.6.5.1.1">
<span class="ltx_p" id="S2.T1.1.6.5.1.1.1" style="width:56.9pt;"><span class="ltx_text" id="S2.T1.1.6.5.1.1.1.1" style="font-size:70%;">Moen Flo</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.6.5.2.1">
<span class="ltx_p" id="S2.T1.1.6.5.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="S2.T1.1.6.5.2.1.1.1" style="font-size:70%;">Water Monitor</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.6.5.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.6.5.3.1">
<span class="ltx_p" id="S2.T1.1.6.5.3.1.1" style="width:71.1pt;"><span class="ltx_text" id="S2.T1.1.6.5.3.1.1.1" style="font-size:70%;">Pattern Analysis</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.6.5.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.6.5.4.1">
<span class="ltx_p" id="S2.T1.1.6.5.4.1.1" style="width:142.3pt;"><span class="ltx_text" id="S2.T1.1.6.5.4.1.1.1" style="font-size:70%;">Usage tracking, inferred habits, data sharing </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.1.6.5.4.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib34" title="">34</a><span class="ltx_text" id="S2.T1.1.6.5.4.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.6.5.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.6.5.5.1">
<span class="ltx_p" id="S2.T1.1.6.5.5.1.1" style="width:85.4pt;"><span class="ltx_text" id="S2.T1.1.6.5.5.1.1.1" style="font-size:70%;">Opaque privacy policies </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.1.6.5.5.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib46" title="">46</a><span class="ltx_text" id="S2.T1.1.6.5.5.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.7.6">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.7.6.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.7.6.1.1">
<span class="ltx_p" id="S2.T1.1.7.6.1.1.1" style="width:56.9pt;"><span class="ltx_text" id="S2.T1.1.7.6.1.1.1.1" style="font-size:70%;">Dyson Pure Cool</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.7.6.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.7.6.2.1">
<span class="ltx_p" id="S2.T1.1.7.6.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="S2.T1.1.7.6.2.1.1.1" style="font-size:70%;">Air Purifier</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.7.6.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.7.6.3.1">
<span class="ltx_p" id="S2.T1.1.7.6.3.1.1" style="width:71.1pt;"><span class="ltx_text" id="S2.T1.1.7.6.3.1.1.1" style="font-size:70%;">Air Quality Analysis</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.7.6.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.7.6.4.1">
<span class="ltx_p" id="S2.T1.1.7.6.4.1.1" style="width:142.3pt;"><span class="ltx_text" id="S2.T1.1.7.6.4.1.1.1" style="font-size:70%;">Air data, inferred health, data sharing </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.1.7.6.4.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib34" title="">34</a><span class="ltx_text" id="S2.T1.1.7.6.4.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.7.6.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.7.6.5.1">
<span class="ltx_p" id="S2.T1.1.7.6.5.1.1" style="width:85.4pt;"><span class="ltx_text" id="S2.T1.1.7.6.5.1.1.1" style="font-size:70%;">Opaque privacy policies </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.1.7.6.5.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib46" title="">46</a><span class="ltx_text" id="S2.T1.1.7.6.5.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.8.7">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.8.7.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.8.7.1.1">
<span class="ltx_p" id="S2.T1.1.8.7.1.1.1" style="width:56.9pt;"><span class="ltx_text" id="S2.T1.1.8.7.1.1.1.1" style="font-size:70%;">Samsung SmartThings</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.8.7.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.8.7.2.1">
<span class="ltx_p" id="S2.T1.1.8.7.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="S2.T1.1.8.7.2.1.1.1" style="font-size:70%;">Automation Platform</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.8.7.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.8.7.3.1">
<span class="ltx_p" id="S2.T1.1.8.7.3.1.1" style="width:71.1pt;"><span class="ltx_text" id="S2.T1.1.8.7.3.1.1.1" style="font-size:70%;">Interoperability</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.8.7.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.8.7.4.1">
<span class="ltx_p" id="S2.T1.1.8.7.4.1.1" style="width:142.3pt;"><span class="ltx_text" id="S2.T1.1.8.7.4.1.1.1" style="font-size:70%;">Device data aggregation, behavioral tracking, surveillance risk </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.1.8.7.4.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib34" title="">34</a><span class="ltx_text" id="S2.T1.1.8.7.4.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.8.7.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.8.7.5.1">
<span class="ltx_p" id="S2.T1.1.8.7.5.1.1" style="width:85.4pt;"><span class="ltx_text" id="S2.T1.1.8.7.5.1.1.1" style="font-size:70%;">Complex privacy settings </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.1.8.7.5.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib46" title="">46</a><span class="ltx_text" id="S2.T1.1.8.7.5.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.9.8">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S2.T1.1.9.8.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.9.8.1.1">
<span class="ltx_p" id="S2.T1.1.9.8.1.1.1" style="width:56.9pt;"><span class="ltx_text" id="S2.T1.1.9.8.1.1.1.1" style="font-size:70%;">Lockly Vision Elite</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S2.T1.1.9.8.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.9.8.2.1">
<span class="ltx_p" id="S2.T1.1.9.8.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="S2.T1.1.9.8.2.1.1.1" style="font-size:70%;">Smart Lock</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S2.T1.1.9.8.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.9.8.3.1">
<span class="ltx_p" id="S2.T1.1.9.8.3.1.1" style="width:71.1pt;"><span class="ltx_text" id="S2.T1.1.9.8.3.1.1.1" style="font-size:70%;">Fingerprint, Facial Recognition</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S2.T1.1.9.8.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.9.8.4.1">
<span class="ltx_p" id="S2.T1.1.9.8.4.1.1" style="width:142.3pt;"><span class="ltx_text" id="S2.T1.1.9.8.4.1.1.1" style="font-size:70%;">Biometric data, security risks, unclear storage </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.1.9.8.4.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib34" title="">34</a><span class="ltx_text" id="S2.T1.1.9.8.4.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S2.T1.1.9.8.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.9.8.5.1">
<span class="ltx_p" id="S2.T1.1.9.8.5.1.1" style="width:85.4pt;"><span class="ltx_text" id="S2.T1.1.9.8.5.1.1.1" style="font-size:70%;">Password, 2FA, firmware updates</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Responsible Innovation and AI Ethics: Guiding Principles for Development</span>
</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.4.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.5.2">Core Ethical Principles for AI</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The ethical use and design of Artificial Intelligence demand adherence to robust ethical guidelines. A generally accepted model identifies five core values: beneficence, non-maleficence, autonomy, justice, and explicability <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib16" title="">16</a>]</cite>. Concepts born in bioethics assist with ethical decision making in the complex environment of AI.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">The beneficence principle implies that AI must promote human welfare, uphold dignity, and assist in preserving the world. It is not about only creating useful products, AI systems must be designed to have beneficial outcomes for humans and other living things <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib21" title="">21</a>]</cite>. This involves ensuring that humans receive fundamental conditions to lead good lives, assisting society to flourish, and maintaining the environment healthy and safe for generations to come <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib24" title="">24</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">Non-maleficence, in contrast to beneficence, relies on the principle that AI systems need to do no harm. It is not just that AI needs to produce good outcomes, but it also needs to ensure that it is not harming anyone. This involves safeguarding user privacy, minimizing security threats, and exercising caution with risky AI technologies that could be abused <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib16" title="">16</a>]</cite>. Chief among these is the fear of an AI arms race, dangers of systems that can self-improve without restraint, and ensuring that AI is kept in safe bounds. Much of this doctrine involves ensuring that developers ensure responsibility for the risks that their technologies may generate and act to mitigate them <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib21" title="">21</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">Autonomy deals with the sensitive balance of decision-making authority between humans and artificial intelligence systems. Implementing AI necessarily means entrusting some degree of decision-making to technology. The concept of autonomy in AI is to maintain that humans should have final authority and the right to choose decisions to outsource, and that such outsourcing should remain reversible <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib16" title="">16</a>]</cite>. There is concern that increasing artificial autonomy could inadvertently undermine human autonomy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib31" title="">31</a>]</cite>. Therefore, the development of AI must promote human autonomy while ensuring that autonomous systems do not infringe upon people’s right to set their own standards and norms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib32" title="">32</a>]</cite>. This concept gives rise to meta-autonomy, also known as a decide-to-delegate paradigm, in which users retain the ultimate authority to decide when and how to cede control, but they also always have the option to override it <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib16" title="">16</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.1">Justice is concerned with ensuring that the rewards of AI are equitably shared and that its application does not exacerbate pre-existing inequalities. This concept upholds development of AI systems promoting fairness globally, combat all forms of discrimination, and ensure equitable access to AI technologies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib12" title="">12</a>]</cite>. It also addresses the issue of imbalanced training data, which can result in biased outcomes, and emphasizes safeguarding systems for social well-being, such as healthcare and social insurance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib21" title="">21</a>]</cite>. Justice also entails applying AI to help rectify past wrongs, promote diversity, and prevent producing new forms of unfairness in society <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib24" title="">24</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p6">
<p class="ltx_p" id="S3.SS1.p6.1">Explicability is defined as an enabling principle that supports the success of the other four ethical principles <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib16" title="">16</a>]</cite>. It focuses on ensuring AI systems are comprehensible and accountable for their decision-making processes, encompassing both intelligibility (understandable to experts and the general public) and accountability (identifying who is responsible for outcomes). Without explicability, it becomes challenging to determine the appropriate level of AI autonomy, assess whether it promotes good (beneficence) or avoids harm (non-maleficence), and hold parties accountable for unfair outcomes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib45" title="">45</a>]</cite>. For vulnerable groups, clear explanations are even more critical, as they may face higher risks if they cannot understand how or why AI systems make decisions. For users like the elderly, children, or neurodivergent individuals, who may have varying AI literacy or unique cognitive processing differences, generic or technically complex explanations may be inadequate <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib33" title="">33</a>]</cite>. If such users cannot comprehend why an agentic AI makes proactive decisions, their autonomy is compromised, potential harms (non-maleficence) are harder to detect, and fairness (justice) issues remain opaque <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib7" title="">7</a>]</cite>. Hence, the literature’s emphasis on explainability and natural language explanations is not merely a feature but a critical ethical necessity. This suggests the need to adapt explanations for diverse user populations, using simpler language, visualizations, or interactive content to ensure true intelligibility, especially for cognitively diverse users <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib8" title="">8</a>]</cite>. The goal is to achieve authentic comprehensibility for all users, extending beyond mere transparency <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib32" title="">32</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.4.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.5.2">Responsible Innovation (RRI) and AI Governance Frameworks</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">When developing technology, especially artificial intelligence, Responsible Innovation (RRI) encourages thinking ahead. It’s meant to guide choices and actions throughout the entire AI development process. Instead of waiting for ethical problems to arise, RRI focuses on protecting and improving human well-being by considering the possible effects of AI right from the beginning and continuing through its design and use <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib18" title="">18</a>]</cite>. This approach also supports involving people with different backgrounds and perspectives early on, while carefully considering a wide range of risks, including long-term and indirect impacts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib34" title="">34</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">IEEE’s Ethically Aligned Design (EAD) initiative is a well-known example of an RRI-based framework that provides specific guidance and recommendations to technologists <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib31" title="">31</a>]</cite>. EAD supports the ethical design, development, and use of autonomous and intelligent systems (A/IS) by prioritizing human well-being and respecting core values such as accountability, transparency, human rights, and avoiding misuse <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib32" title="">32</a>]</cite>. To ensure that AI is used responsibly and that its operations can be monitored to foster public trust, it firmly supports the establishment of open governance frameworks, technical specifications, and regulatory bodies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib13" title="">13</a>]</cite>. The framework also highlights the importance of understanding community norms, incorporating human values into AI systems, and regularly evaluating the performance of these systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib17" title="">17</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">Numerous AI governance frameworks have been developed worldwide to foster public confidence, ensure accountability, and support the moral use of AI. These initiatives include the AI Act of the European Union, the Research, Innovation and Accountability Act draft and the NAIAC AI Transparency Report of the United States, Japan’s soft law approach, Canada’s Directive on Automated Decision-Making, and China’s AI Governance Principles <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib37" title="">37</a>]</cite>. In contrast to their differing methodologies, all of these frameworks have a number of important ideas in common, for instance the requirement for varying levels of transparency according to system risk, frequent updating of documents during development, and the provision of explanations specific to various groups of stakeholders <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib17" title="">17</a>]</cite>. The EU AI Act, for example, classifies AI systems according to their level of risk and places stringent requirements on high-risk systems, such as guidelines for risk management, transparency, and human intervention.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">Additionally, it gives people the right to know how decisions that affect them are made <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib21" title="">21</a>]</cite>. Similarly, IEEE-USA is also in favor of building AI systems which are safe, ethical, trustworthy, and in line with democratic principles. It emphasizes the need for accountability and transparency in the use of AI by the government and demands periodic audits to ensure proper utilization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib31" title="">31</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1">An increasingly pronounced trend toward AI governance points to a clear departure from post-facto, reactive regulation in favor of a more proactive ethics-by-design approach. Ethically Aligned Design (EAD) and Responsible Research and Innovation (RRI) are two models that strongly support integrating ethical thinking into all stages of AI development, ideally starting before technical development begins <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib18" title="">18</a>]</cite>. This shift acknowledges that addressing ethical issues like bias or lack of transparency post-development is challenging due to system complexity <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib32" title="">32</a>]</cite>. Developers can create trustworthy and responsible AI by incorporating ethics from the outset using value-based system design <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib31" title="">31</a>]</cite>. The literature advocates RRI-based ethical principles and well-specified control points for user override and transparency <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib21" title="">21</a>]</cite>. Ethics are not treated as a checklist to be addressed later but as an integral aspect of system conceptualization and operation. This also involves engaging experts across disciplines from the start, such as ethicists, social scientists, and end-users, collaborating with technical teams to develop systems collectively <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib38" title="">38</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Human-Centered and Participatory Design in AI for Vulnerable Populations</span>
</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.4.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.5.2">Principles of Human-Centered AI (HCAI)</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Bias in AI systems is a serious ethical concern, especially in sensitive areas like smart homes. In these systems, bias can appear in many parts of the machine learning process, such as the algorithm’s parameters, model weights, or the training data used <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib21" title="">21</a>]</cite>. This can lead to decisions that are unfair or unequal, particularly when they affect people based on factors like age, gender, income, health, ethnicity, or religion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib5" title="">5</a>]</cite>. While many studies look at finding and measuring bias in the technical parts of AI, focusing only on this can sometimes make us miss the real-world problem: discrimination and its harmful effects <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib12" title="">12</a>]</cite>. This is already apparent in other settings; for example, some city-funded camera systems have shown bias in identifying people, and facial recognition software often works better for users from specific regions than others. These examples show how bias in AI can have unfair and harmful effects.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">HCAI is a core methodology to AI development that puts human well-being and social benefit at the center. The framework aims to enhance AI benefits while diligently diminishing its harms by systematically analyzing implications on end-users, service providers, impacted communities, and society as a whole <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib56" title="">56</a>]</cite>. The HCAI approach addresses several key dimensions: making AI development processes more refined in order to avoid failures and adverse effects, designing novel human-AI interaction methods, enhancing public awareness of AI, developing well-designed AI policies and regulations, and enhancing human-AI collaboration to facilitate mutual innovation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib12" title="">12</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">HCAI offers a way of mitigating these biases. It emphasizes engaging diverse groups, including HCD experts, ethicists, patients, and community members, throughout the end-to-end AI development process <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib56" title="">56</a>]</cite>. Through this, biases can be detected and corrected early, from data labeling and collection to model design, development, and application in the real world <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib12" title="">12</a>]</cite>. Important strategies include implementing comprehensive bias-detection techniques, ensuring that models perform well across all groups, using training data that represents a variety of populations, and involving a range of annotators to reduce individual biases <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib38" title="">38</a>]</cite>. The goal is to create safe, equitable, and inclusive AI systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib7" title="">7</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">The HCAI methodology uses machine learning in a manner that highly respects human needs, behaviors, and social contexts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib56" title="">56</a>]</cite>. This needs to involve collaboration among experts from a wide range of disciplines - including human-centered design specialists, ethicists, social scientists, legal consultants, health care practitioners, AI experts, teachers, communication scholars, and above all, patients and community members <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib35" title="">35</a>]</cite>. It is only by this collaborative diversity that user can effectively recognize and overcome biases at each step, from collecting and tagging data to model design, system testing, and implementation, so the resulting AI functions equitably and safely for all <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib5" title="">5</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1">The problem of bias in AI systems goes beyond just technical issues, showing how discrimination is deeply connected to both social and technical factors. While technical solutions like adjusting datasets or using counterfactual methods are important for fixing bias in parameters, model weights, and data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib56" title="">56</a>]</cite>, discrimination often emerges as an unintended consequence with critical outcomes in real-world applications <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib21" title="">21</a>]</cite>. This shows that bias isn’t just a technical challenge - it’s fundamentally linked to social norms, how researchers collect human data, and interpret complex human behavior.</p>
</div>
<div class="ltx_para" id="S4.SS1.p6">
<p class="ltx_p" id="S4.SS1.p6.1">Surveys and data scraping from sites such as Reddit, Twitter now X, and YouTube comments (Phase 2) provide rich insights into user sentiment and experience <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib58" title="">58</a>]</cite>. It is, however, essential to appreciate that social media data tends to be pre-loaded with biases, even when employing sophisticated NLP mechanisms like BERTopic, VADER, or GPT-style models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib44" title="">44</a>]</cite>. Such AI tools can still fail to fully understand the complex, subtle connotations, deeper meanings, and emotional nuances <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib11" title="">11</a>]</cite> that are necessary for pinpointing moral issues or the unmet needs of vulnerable groups.</p>
</div>
<div class="ltx_para" id="S4.SS1.p7">
<p class="ltx_p" id="S4.SS1.p7.1">Because of the shortcomings of purely technical approaches, participatory design techniques are essential. Directly involving vulnerable populations in identifying and addressing potential discriminatory outcomes in AI systems can yield valuable insights that automated data analysis alone cannot <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib38" title="">38</a>]</cite>. These community members contribute firsthand knowledge and lived experiences to ensure that AI development remains authentically aligned with real human needs and ethical values <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib8" title="">8</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p8">
<p class="ltx_p" id="S4.SS1.p8.1">Human-Centered AI (HCAI) is a continuous, cyclical endeavor to promote trust and facilitate the broad adoption of AI systems. According to IEEE’s values of Responsible Innovation (RRI) and Ethically Aligned Design (EAD), ethical integration should take place throughout the lifecycle of AI development, ideally prior to development even beginning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib18" title="">18</a>]</cite>. This continuous engagement serves two crucial purposes: enhancing system performance and fostering public trust, as transparency and comprehension are the cornerstones of trustworthy AI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib17" title="">17</a>]</cite>. The iterative HCAI approach explicitly integrates user feedback, particularly from target groups, into every development stage. This approach supports continuous refinement across development stages, ensuring that AI technologies remain sensitive to human values and needs, thus maximizing their beneficial impacts on society and minimizing potential harms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib21" title="">21</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.4.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.5.2">Participatory Design: Opportunities and Challenges for Inclusive AI</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Participatory Design (PD), which is a classic Human-Computer Interaction (HCI) methodology, is becoming more and more prevalent in shaping the design of AI systems. PD essentially implies the direct participation of a wide range of actors, in particular the end user, in several stages of the design process. These participants worked together with researchers and designers in a participatory manner to support collective decision-making about which kinds of AI were designed/implemented and to do so in direct relation to their daily lives <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib35" title="">35</a>]</cite>. This methodology is particularly worth it in order to deal with some of the most significant issues in AI, including algorithmic bias, transparency, and the proliferation of disinformation, all of which typically affect marginalized populations disproportionately <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib5" title="">5</a>]</cite>. By involving users from the early stage of AI systems’ design process in a meaningful way, PD makes sure that AI systems are grounded on human values right from the beginning. It also leads to more informed decision-making, by making sure that technological solutions better align with real needs, and as such deliver fairer and more empowering results <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib38" title="">38</a>]</cite>. PD effects are being felt in multiple locations, for example by improving gig worker working conditions with co-designed AI tools, through to making children’s online safety better <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib8" title="">8</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">While Participatory Design (PD) has numerous advantages, integrating it into AI-based technologies also poses significant challenges. The most prominent is whether PD can feasibly be incorporated into the sometimes intricate workflows of AI development <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib35" title="">35</a>]</cite>. Researchers often find it difficult to decide which aspects of an AI system, i.e., the user interface, explanations, machine learning models, or training data, are best suited for co-design, particularly in the case of fixed datasets or general-purpose systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib5" title="">5</a>]</cite>. Another problem is ensuring meaningful participation. Authentic PD is more than occasional workshops; it involves active, sustained participation in which participants really have control over the system design and outcome <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib12" title="">12</a>]</cite>. There are also practical concerns, like figuring out how to implement the ideas and artifacts produced through co-design, determining ownership of rights to designs developed by non-researchers, and dealing with the challenge of working across disparate kinds of expertise. For example, participants who are not from technical backgrounds may find it difficult to participate due to the use of specialized technical language <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib7" title="">7</a>]</cite>. Finally, while commercial AI systems are usually made for global use, PD tends to focus on local communities. Scaling participatory approaches may be difficult due to this incompatibility <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib38" title="">38</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">There is not only need, but a need now to close the participation gap for people who are currently marginalized in AI development. Participatory Design has been described as essential for preventing negative effects, especially to those most at risk <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib35" title="">35</a>]</cite>, but there are indeed real barriers. Computer jargon and low exposure to AI concepts can stifle or push these users to the sidelines in terms of meaningful participation in the design process <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib8" title="">8</a>]</cite>. The increasing participatory turn in AI ethics highlights the importance of public responsibility in AI development, especially in terms of the disparate harms such systems inflict on vulnerable groups <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib38" title="">38</a>]</cite>. This involves employing inclusive practices such as arts-and-crafts-based ideation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib35" title="">35</a>]</cite>, physical prototyping, or other hands-on tools that don’t require technical know-how. It also includes demystifying obtuse AI concepts and creating safe, judgment-free spaces where vulnerable customers feel actively heard. Feedback is not just being gathered, but an approach which co-designs systems built by the very populations they serve, and dictates how AI behaves, its ethics, and the experience itself in a first-person lived experience <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib14" title="">14</a>]</cite>. This is particularly important for neurodivergent users, whose outside-the-box thinking and unique viewpoints can assist in bringing up ethics blind spots and edge cases that mainstream design teams may overlook <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib7" title="">7</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.4.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.5.2">Designing for Inclusivity: General Considerations for Diverse Users</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Inclusive design is a guiding principle that encourages designing products and systems for everyone, irrespective of their background, ability, or demographics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib33" title="">33</a>]</cite>. In the context of AI, it is the key to empowering diverse groups and bridging societal divides. As an example, inclusive AI in leadership support tools means that leaders from various cultural, linguistic, or socioeconomic contexts are able to easily interact with such tools, making organizational environments more inclusive and efficient <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib5" title="">5</a>]</cite>. Cultural differences or language disparities can be overcome by well-considered design decisions, such as multilingual interfaces and culturally sensitive features <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib33" title="">33</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">Most importantly, inclusive AI design is actually done through collaboration with the very communities that will be impacted by it. This co-creation follows from user insight to inform the creation of content and features, making the system relevant and accessible. Such a model has worked successfully in partnerships with community health workers, where solutions were developed in collaboration with those most affected by the technology <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib2" title="">2</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">Inclusive design of AI extends far beyond physical accessibility, but also involves carefully addressing cognitive and cultural differences in human-AI interactions. Multilingual support and cultural sensitivity are crucial <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib33" title="">33</a>]</cite>, but inclusivity also has to address differences in cognitive capacity and AI competence, particularly among vulnerable populations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib38" title="">38</a>]</cite>. A system built with neurotypical adults in mind may be confusing or daunting to a neurodivergent child or someone who is elderly and suffering from cognitive decline <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib10" title="">10</a>]</cite>. Therefore, it is essential to develop with a broad variety of cognitive styles and cultural expectations. AI Design should present adaptable interaction alternatives, e.g., voice, text, or reduced graphical interfaces. It should also enable users to manage how much information they get and how rapidly it arrives. For instance, an older adult’s AI assistant could use slower conversation speeds and repeated, straightforward confirmations, while a neurodivergent user’s AI could minimize sudden interruptions or provide sensory accommodations such as sound modulation or adaptive lighting to prevent overstimulation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib8" title="">8</a>]</cite>. By adopting this degree of subtlety, AI systems can be made not only accessible, but actually usable and empowering for individuals along the entire range of human diversity <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib12" title="">12</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Ethical Alignment Features: Explainability, Consent, and Override Mechanisms</span>
</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.4.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.5.2">Explainable AI (XAI) for Agentic Systems: Techniques and Importance</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Explainable Artificial Intelligence (XAI)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib57" title="">57</a>]</cite> is a key area that works to make AI clear, transparent, and trusted. This is very key for AI that acts on its own, where users must get how it makes choices to make sure it is fair and to grow trust. This is true in big deal areas like health, money, or law<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib1" title="">1</a>]</cite>. Today’s deep learning tech, even if very good, can often be like a black boxes it can’t be seen how it works or why it picks what it picks. XAI<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib57" title="">57</a>]</cite> works to fill this gap, helping us get how and why an AI does what it does. This lets users check it by himself with independent evaluation and helps build trust.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">Large Language Models (LLMs)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib30" title="">30</a>]</cite> enhances XAI<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib57" title="">57</a>]</cite> by transforming complex machine learning outputs into narratives that are easy for humans to understand <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib45" title="">45</a>]</cite>. This approach aims to make model predictions more accessible and to bridge the interpretability gap between sophisticated model behavior and human comprehension <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib30" title="">30</a>]</cite>. Several XAI<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib57" title="">57</a>]</cite> techniques are used to provide the insights. LIME (Local Interpretable Model agnostic Explanations)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib36" title="">36</a>]</cite> and SHAP (SHapley Additive exPlanations)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib28" title="">28</a>]</cite> are two widely used methods that generate local explanations for predictions made by black-box models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib20" title="">20</a>]</cite>. LIME<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib36" title="">36</a>]</cite> works by perturbing the input and training a simple, interpretable model on these perturbed instances to approximate the black-box model’s behavior locally, while SHAP<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib28" title="">28</a>]</cite> attributes the contribution of each feature to the prediction based on cooperative game theory <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib38" title="">38</a>]</cite>. Other XAI<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib57" title="">57</a>]</cite> approaches includes rule extraction, showing what-if scenarios, saliency maps, and attention mechanisms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib58" title="">58</a>]</cite>. Challenges in XAI development include balancing the trade-off between model accuracy and explainability, ensuring scalability for large models, and guaranteeing that explanations are both accurate and practically useful for diverse users <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib45" title="">45</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">The challenge of giving clear and simple reasons for AI actions to people of all learning levels is key. This is very important for groups like old people, kids, and neurodivergent individuals. Although deep AI tools such as LIME<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib36" title="">36</a>]</cite> and SHAP<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib28" title="">28</a>]</cite> give important insights into how AI works, this service is for these groups of people who may not know a lot about AI<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib20" title="">20</a>]</cite>. This can make it hard for them to get what the AI is doing<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib33" title="">33</a>]</cite>. Research indicates that generic feature-based explanations may not significantly improve user accuracy or be easily understandable, and even counterfactual explanations can be perceived as complex <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib38" title="">38</a>]</cite>. This shows that just showing how a model works isn’t enough; the aim should be to turn complex machine learning results into simple stories. So, XAI<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib45" title="">45</a>]</cite> systems need to be made to give clear explanations to different groups of people, using human input to stay flexible<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib29" title="">29</a>]</cite>. For people who find AI tech hard, this meansfocusing on what an AI does, more than how it does it<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib33" title="">33</a>]</cite>. A good mix of detail and simple talk is required, so all can get it, no matter how they think. Using different ways to explain (like voice, easy words, or pictures) and letting users choose how much they want to know can make things easier to use <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib50" title="">50</a>]</cite>. Saying things in plain language is a good move, but how deep and in what style it explain must change to fit each person’s understanding of AI and their situation<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib12" title="">12</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS2.4.1.1">V-B</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS2.5.2">Designing for User Consent and Data Privacy in Proactive AI</span>
</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">By their very nature, proactive AI systems require the gathering and processing of large amounts of personal data in order to predict user needs and carry out autonomous actions, which raises serious privacy concerns. Explicit consent for data processing, the right to erasure, and data portability are crucial, according to international laws like the California Privacy Rights Act (CCPA/CPRA) in the US and the General Data Protection Regulation (GDPR) in the EU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib46" title="">46</a>]</cite>. The AI Act of the European Union further divides AI systems into risk categories and requires strict transparency and human supervision for high-risk applications <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib17" title="">17</a>]</cite>. These legal frameworks emphasise how important it is to be transparent about data sources and processing in order to foster trust and guarantee accountability <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib21" title="">21</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">Within Human-Computer Interaction (HCI), the concept of consent is paramount for designing safe and agentic computer-mediated communication. It extends beyond mere legal compliance to encompass fundamental issues of data privacy, security, and the ethical dynamics of human-robot interaction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib3" title="">3</a>]</cite>. True consent is understood as being about human beings’ safety and agency, moving beyond the simplistic notion of satisfying a formality through simple check boxes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib21" title="">21</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">The limitations of existing consent procedures are often demonstrated by real-world smart home device deployments. Continuous voice recordings, widespread video surveillance, and the sharing of user data with third parties or law enforcement-often without explicit, detailed consent or readily accessible opt-out mechanisms-have raised privacy concerns about products like Amazon Echo and Ring <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib34" title="">34</a>]</cite>. Users frequently don’t fully understand their privacy rights and the wide range of data collection methods these devices use <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib58" title="">58</a>]</cite>. The loss of user control over their own data is exemplified by Amazon’s policy changes, which mandate that all Echo voice recordings be sent for analysis regardless of user preference <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib46" title="">46</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1">The transition from click-through consent to granular, contextual, and ongoing consent is a critical design requirement for proactive AI. The current state reveals that traditional consent processes, buried in lengthy privacy notices or ambiguous settings, are too frequently ineffectual, leading to a breakdown in meaningful consent and a loss of user trust in turn <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib34" title="">34</a>]</cite>. Active AI, which acts on behalf of the user based on inferred preference from gathered data, fundamentally repositions the nature of consent from a one-off agreement to collect data to an ongoing, dynamic consent to action. Therefore, the system’s consent prompts should be designed not as solitary agreements but as dynamic, contextual interactions that are performed prior to a proactive decision being made, especially for sensitive behavior (e.g., disclosure of health information to caregivers) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib10" title="">10</a>]</cite>. Consent has to be granular so that users can be in control of certain kinds of data or actions rather than an all-or-nothing approach. User Interface/User Experience (UI/UX) designs such as Data Privacy Controls offering fine-grained choices, opt-in/opt-out clear controls, and export or delete user data possibilities are essential to facilitate this <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib34" title="">34</a>]</cite>. Also, Governors and Trust indicators UI/UX patterns <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib9" title="">9</a>]</cite> can help maintain user agency and foster trust by ensuring AI operations become transparent (e.g., citations, controls, footprints, prompt transparency, incognito mode). This trajectory leads towards a decide-to-delegate approach, where humans retain final control of what decisions need to be taken by the AI and when.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS3.4.1.1">V-C</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS3.5.2">User Override and Control Mechanisms in Autonomous Systems</span>
</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">The growing independence of AI systems, especially agentic AI, creates an essential requirement for strong user override and control features. Studies show that over-reliance on AI software may reduce human critical thinking engagement and autonomous problem-solving ability, and thus leave people vulnerable to unforeseen circumstances or exceptions not addressed by the AI 
<br class="ltx_break"/>citegranny. This underlines the importance of creating systems that not only perform tasks automatically but also enable users to retain meaningful control.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">AI system architecture designs usually contain elements regarding security and safety and architecture as a whole to fix these problems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib34" title="">34</a>]</cite>. Of note in this field is Human-in-the-Loop (HITL) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib29" title="">29</a>]</cite>, which positions human managers at the focal point of guiding and augmenting AI systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib54" title="">54</a>]</cite>. HITL systems<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib29" title="">29</a>]</cite> purposefully inject human oversight, judgment, and accountability into the AI process, providing directed opportunities for intervention, guidance, and control at various points of operation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib32" title="">32</a>]</cite>. This can be in the form of explicit wait-for-human processes, where AI pauses and requests human input before proceeding; approval pipelines, where human approval targets outputs generated by AI; and active learning &amp; feedback loops, where human feedback is used to update and refine AI algorithms in a continuous manner <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib54" title="">54</a>]</cite>. Good user interface design is crucial to enable this human oversight, requiring UIs and APIs to present information clearly, contextualized, and hierarchically to enable situational awareness and well-informed decision-making <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib38" title="">38</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1">Design patterns are critical to enable trusted services and user empowerment by introducing just enough friction to enable visibility into the underlying system and enable users to understand how and why a service has changed <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib34" title="">34</a>]</cite>. Trust can be made more difficult by AI through not being able to distinguish human from bot, abrupt UI modifications, and users possessing erroneous mental models of how an AI system operates <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib21" title="">21</a>]</cite>. Therefore, override mechanisms are not just stopping an action but restoring user control and recovering trust.</p>
</div>
<div class="ltx_para" id="S5.SS3.p4">
<p class="ltx_p" id="S5.SS3.p4.1">Designing for high-level human control based on interruptibility and reversibility is a fundamental requirement of agentic AI systems. The papers emphasize the need for override mechanisms, towards the risks of over-trust and the function of human intervention to supply security and responsibility <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib54" title="">54</a>]</cite>. It is not a case of a simple off switch but a nuanced approach to human control <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib31" title="">31</a>]</cite>. Human-in-the-loop (HITL) trends offer concrete technical solutions for its implementation, integrating human inspection and approval into pivotal decision-making points <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib29" title="">29</a>]</cite>. For agentic AI, override controls must be discoverable, understandable, and effective. That is, UI/UX that makes direct interruption and, if possible, reversal of AI action straightforward for proactive decisions. It also involves providing good feedback on the state of the AI and the consequences of an override <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib32" title="">32</a>]</cite>. The problem is to install these controls without inducing fatigue or excessive restriction of the AI’s useful autonomy. UI/UX patterns such as ’Show the work’ or Footprints <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib34" title="">34</a>]</cite> can support this by making the AIs reasoning transparent enough for users to make informed decisions about when and how to intervene. It can be seen in detail about Override and Interruption Mechanism in Table <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S5.T2" title="TABLE II ‣ V-C User Override and Control Mechanisms in Autonomous Systems ‣ V Ethical Alignment Features: Explainability, Consent, and Override Mechanisms ‣ Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_tag">II</span></a>.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Technical Design Patterns for AI Override and Interruption Mechanisms</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S5.T2.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.1.1.1.1">
<span class="ltx_p" id="S5.T2.1.1.1.1.1.1" style="width:62.6pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1.1.1.1" style="font-size:70%;">Pattern Category</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S5.T2.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.1.1.2.1">
<span class="ltx_p" id="S5.T2.1.1.1.2.1.1" style="width:76.8pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.2.1.1.1" style="font-size:70%;">Specific Pattern</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S5.T2.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.1.1.3.1">
<span class="ltx_p" id="S5.T2.1.1.1.3.1.1" style="width:91.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.3.1.1.1" style="font-size:70%;">Mechanism</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S5.T2.1.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.1.1.4.1">
<span class="ltx_p" id="S5.T2.1.1.1.4.1.1" style="width:105.3pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.4.1.1.1" style="font-size:70%;">Benefits</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S5.T2.1.1.1.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.1.1.5.1">
<span class="ltx_p" id="S5.T2.1.1.1.5.1.1" style="width:105.3pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.5.1.1.1" style="font-size:70%;">Challenges</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" colspan="5" id="S5.T2.1.2.2.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.2.2.1.1" style="font-size:70%;">Human-in-the-Loop (HITL)</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.3.3">
<td class="ltx_td ltx_align_top" id="S5.T2.1.3.3.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.3.3.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.3.3.2.1">
<span class="ltx_p" id="S5.T2.1.3.3.2.1.1" style="width:76.8pt;"><span class="ltx_text" id="S5.T2.1.3.3.2.1.1.1" style="font-size:70%;">Elicitation Middleware </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T2.1.3.3.2.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib54" title="">54</a><span class="ltx_text" id="S5.T2.1.3.3.2.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.3.3.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.3.3.3.1">
<span class="ltx_p" id="S5.T2.1.3.3.3.1.1" style="width:91.0pt;"><span class="ltx_text" id="S5.T2.1.3.3.3.1.1.1" style="font-size:70%;">AI pauses, requests user input/validation.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.3.3.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.3.3.4.1">
<span class="ltx_p" id="S5.T2.1.3.3.4.1.1" style="width:105.3pt;"><span class="ltx_text" id="S5.T2.1.3.3.4.1.1.1" style="font-size:70%;">Ensures human judgment; prevents unintended actions.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.3.3.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.3.3.5.1">
<span class="ltx_p" id="S5.T2.1.3.3.5.1.1" style="width:105.3pt;"><span class="ltx_text" id="S5.T2.1.3.3.5.1.1.1" style="font-size:70%;">Increases latency; needs clear state communication; risk of user fatigue.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.4.4">
<td class="ltx_td ltx_align_top" id="S5.T2.1.4.4.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.4.4.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.4.4.2.1">
<span class="ltx_p" id="S5.T2.1.4.4.2.1.1" style="width:76.8pt;"><span class="ltx_text" id="S5.T2.1.4.4.2.1.1.1" style="font-size:70%;">Approval Pipelines </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T2.1.4.4.2.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib54" title="">54</a><span class="ltx_text" id="S5.T2.1.4.4.2.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.4.4.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.4.4.3.1">
<span class="ltx_p" id="S5.T2.1.4.4.3.1.1" style="width:91.0pt;"><span class="ltx_text" id="S5.T2.1.4.4.3.1.1.1" style="font-size:70%;">AI outputs routed for human review (approve, reject, edit).</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.4.4.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.4.4.4.1">
<span class="ltx_p" id="S5.T2.1.4.4.4.1.1" style="width:105.3pt;"><span class="ltx_text" id="S5.T2.1.4.4.4.1.1.1" style="font-size:70%;">Structured gate for sensitive outputs; boosts accountability.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.4.4.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.4.4.5.1">
<span class="ltx_p" id="S5.T2.1.4.4.5.1.1" style="width:105.3pt;"><span class="ltx_text" id="S5.T2.1.4.4.5.1.1.1" style="font-size:70%;">Bottlenecks; needs efficient review interfaces; inconsistent feedback risk.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.5.5">
<td class="ltx_td ltx_align_top" id="S5.T2.1.5.5.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.5.5.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.5.5.2.1">
<span class="ltx_p" id="S5.T2.1.5.5.2.1.1" style="width:76.8pt;"><span class="ltx_text" id="S5.T2.1.5.5.2.1.1.1" style="font-size:70%;">Active Learning </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T2.1.5.5.2.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib54" title="">54</a><span class="ltx_text" id="S5.T2.1.5.5.2.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.5.5.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.5.5.3.1">
<span class="ltx_p" id="S5.T2.1.5.5.3.1.1" style="width:91.0pt;"><span class="ltx_text" id="S5.T2.1.5.5.3.1.1.1" style="font-size:70%;">Human overrides refine AI algorithms as training data.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.5.5.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.5.5.4.1">
<span class="ltx_p" id="S5.T2.1.5.5.4.1.1" style="width:105.3pt;"><span class="ltx_text" id="S5.T2.1.5.5.4.1.1.1" style="font-size:70%;">Improves accuracy, user alignment over time.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.5.5.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.5.5.5.1">
<span class="ltx_p" id="S5.T2.1.5.5.5.1.1" style="width:105.3pt;"><span class="ltx_text" id="S5.T2.1.5.5.5.1.1.1" style="font-size:70%;">Needs robust data infrastructure; risk of propagating biases.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.6.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" colspan="5" id="S5.T2.1.6.6.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.6.6.1.1" style="font-size:70%;">Governors (UI/UX)</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.7.7">
<td class="ltx_td ltx_align_top" id="S5.T2.1.7.7.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.7.7.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.7.7.2.1">
<span class="ltx_p" id="S5.T2.1.7.7.2.1.1" style="width:76.8pt;"><span class="ltx_text" id="S5.T2.1.7.7.2.1.1.1" style="font-size:70%;">Controls </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T2.1.7.7.2.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib34" title="">34</a><span class="ltx_text" id="S5.T2.1.7.7.2.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.7.7.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.7.7.3.1">
<span class="ltx_p" id="S5.T2.1.7.7.3.1.1" style="width:91.0pt;"><span class="ltx_text" id="S5.T2.1.7.7.3.1.1.1" style="font-size:70%;">UI elements to manage flow, pause, or adjust AI prompts.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.7.7.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.7.7.4.1">
<span class="ltx_p" id="S5.T2.1.7.7.4.1.1" style="width:105.3pt;"><span class="ltx_text" id="S5.T2.1.7.7.4.1.1.1" style="font-size:70%;">Enhances real-time user agency, control.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.7.7.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.7.7.5.1">
<span class="ltx_p" id="S5.T2.1.7.7.5.1.1" style="width:105.3pt;"><span class="ltx_text" id="S5.T2.1.7.7.5.1.1.1" style="font-size:70%;">Must be intuitive; balancing control vs. automation.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.8.8">
<td class="ltx_td ltx_align_top" id="S5.T2.1.8.8.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.8.8.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.8.8.2.1">
<span class="ltx_p" id="S5.T2.1.8.8.2.1.1" style="width:76.8pt;"><span class="ltx_text" id="S5.T2.1.8.8.2.1.1.1" style="font-size:70%;">Footprints </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T2.1.8.8.2.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib34" title="">34</a><span class="ltx_text" id="S5.T2.1.8.8.2.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.8.8.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.8.8.3.1">
<span class="ltx_p" id="S5.T2.1.8.8.3.1.1" style="width:91.0pt;"><span class="ltx_text" id="S5.T2.1.8.8.3.1.1.1" style="font-size:70%;">Audit trail of AI’s decision steps from prompt to result.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.8.8.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.8.8.4.1">
<span class="ltx_p" id="S5.T2.1.8.8.4.1.1" style="width:105.3pt;"><span class="ltx_text" id="S5.T2.1.8.8.4.1.1.1" style="font-size:70%;">Supports transparency, trust, debugging.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.8.8.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.8.8.5.1">
<span class="ltx_p" id="S5.T2.1.8.8.5.1.1" style="width:105.3pt;"><span class="ltx_text" id="S5.T2.1.8.8.5.1.1.1" style="font-size:70%;">Complex for non-experts; needs clear visualization.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.9.9">
<td class="ltx_td ltx_align_top" id="S5.T2.1.9.9.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.9.9.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.9.9.2.1">
<span class="ltx_p" id="S5.T2.1.9.9.2.1.1" style="width:76.8pt;"><span class="ltx_text" id="S5.T2.1.9.9.2.1.1.1" style="font-size:70%;">Prompt Transparency </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T2.1.9.9.2.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib34" title="">34</a><span class="ltx_text" id="S5.T2.1.9.9.2.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.9.9.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.9.9.3.1">
<span class="ltx_p" id="S5.T2.1.9.9.3.1.1" style="width:91.0pt;"><span class="ltx_text" id="S5.T2.1.9.9.3.1.1.1" style="font-size:70%;">AI shows internal prompt or planned steps before execution.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.9.9.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.9.9.4.1">
<span class="ltx_p" id="S5.T2.1.9.9.4.1.1" style="width:105.3pt;"><span class="ltx_text" id="S5.T2.1.9.9.4.1.1.1" style="font-size:70%;">Confirms intent; enables pre-emptive intervention.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.9.9.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.9.9.5.1">
<span class="ltx_p" id="S5.T2.1.9.9.5.1.1" style="width:105.3pt;"><span class="ltx_text" id="S5.T2.1.9.9.5.1.1.1" style="font-size:70%;">Increases cognitive load; needs concise presentation.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.10.10">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" colspan="5" id="S5.T2.1.10.10.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.10.10.1.1" style="font-size:70%;">Interruption/Reversal</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.11.11">
<td class="ltx_td ltx_align_top" id="S5.T2.1.11.11.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.11.11.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.11.11.2.1">
<span class="ltx_p" id="S5.T2.1.11.11.2.1.1" style="width:76.8pt;"><span class="ltx_text" id="S5.T2.1.11.11.2.1.1.1" style="font-size:70%;">Regenerate </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T2.1.11.11.2.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib34" title="">34</a><span class="ltx_text" id="S5.T2.1.11.11.2.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.11.11.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.11.11.3.1">
<span class="ltx_p" id="S5.T2.1.11.11.3.1.1" style="width:91.0pt;"><span class="ltx_text" id="S5.T2.1.11.11.3.1.1.1" style="font-size:70%;">User requests new or alternative AI outputs.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.11.11.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.11.11.4.1">
<span class="ltx_p" id="S5.T2.1.11.11.4.1.1" style="width:105.3pt;"><span class="ltx_text" id="S5.T2.1.11.11.4.1.1.1" style="font-size:70%;">Offers flexibility for unsatisfactory results.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T2.1.11.11.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.11.11.5.1">
<span class="ltx_p" id="S5.T2.1.11.11.5.1.1" style="width:105.3pt;"><span class="ltx_text" id="S5.T2.1.11.11.5.1.1.1" style="font-size:70%;">Limited if core reasoning is flawed; variation quality varies.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.12.12">
<td class="ltx_td ltx_align_top ltx_border_bb" id="S5.T2.1.12.12.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S5.T2.1.12.12.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.12.12.2.1">
<span class="ltx_p" id="S5.T2.1.12.12.2.1.1" style="width:76.8pt;"><span class="ltx_text" id="S5.T2.1.12.12.2.1.1.1" style="font-size:70%;">Safety-by-Design </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T2.1.12.12.2.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib31" title="">31</a><span class="ltx_text" id="S5.T2.1.12.12.2.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S5.T2.1.12.12.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.12.12.3.1">
<span class="ltx_p" id="S5.T2.1.12.12.3.1.1" style="width:91.0pt;"><span class="ltx_text" id="S5.T2.1.12.12.3.1.1.1" style="font-size:70%;">Ethical considerations integrated into AI design.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S5.T2.1.12.12.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.12.12.4.1">
<span class="ltx_p" id="S5.T2.1.12.12.4.1.1" style="width:105.3pt;"><span class="ltx_text" id="S5.T2.1.12.12.4.1.1.1" style="font-size:70%;">Prevents harm via built-in safety; reduces override needs.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S5.T2.1.12.12.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.12.12.5.1">
<span class="ltx_p" id="S5.T2.1.12.12.5.1.1" style="width:105.3pt;"><span class="ltx_text" id="S5.T2.1.12.12.5.1.1.1" style="font-size:70%;">Requires early ethical research, collaboration.</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Tailoring Agentic AI for Specific Vulnerable User Scenarios</span>
</h2>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS1.4.1.1">VI-A</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS1.5.2">Agentic AI for Elderly Users: Support, Privacy, and Autonomy</span>
</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">AI assistants can help make life easier and provide social support for older adults, but using them raises important ethical issues like privacy, security, and maintaining independence. In long-term care facilities, AI tools such as wearable devices, room sensors, and social robots are being used <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib43" title="">43</a>]</cite>. Social robots such as PARO<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib42" title="">42</a>]</cite> are mostly popular since they offer companionship, track health indicators, or entertain <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib22" title="">22</a>]</cite>. On the other hand, individuals are ambivalent towards room sensors and wearable technology because they are monitored constantly and they feel uneasy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib11" title="">11</a>]</cite>. Research on the effectiveness with which these devices enhance health is mixed, with a majority having high risk of bias <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib10" title="">10</a>]</cite>. As illustrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S6.T3" title="TABLE III ‣ VI-C Agentic AI for Neurodivergent Users: Accessibility and Empowerment ‣ VI Tailoring Agentic AI for Specific Vulnerable User Scenarios ‣ Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_tag">III</span></a>, prominent ethical issues and proposed design strategies with their corresponding frameworks have been addressed.</p>
</div>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1">Using AI raises important ethical concerns for older adults. For instance, they might think a robot is a real pet or feel like they’re being treated like children, which can hurt their sense of dignity <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib43" title="">43</a>]</cite>. Another issue is the sadness older adults might feel if they become fond of a robot and it is removed <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib12" title="">12</a>]</cite>. AI sensors that monitor constantly can make care feel less personal and raise worries about data privacy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib43" title="">43</a>]</cite>. AI assistants learn what users like and adjust their responses, which makes them easier to use but creates challenges in protecting private information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib10" title="">10</a>]</cite>. Older adults often prefer simple, mistake-free systems and want clear control over how their personal information is shared, especially with caregivers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib21" title="">21</a>]</cite>. A major obstacle to trust is that data privacy policies are often hard to find and understand for older adults <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib16" title="">16</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS1.p3">
<p class="ltx_p" id="S6.SS1.p3.1">AI systems can provide important benefits, like offering social support and companionship, which help reduce loneliness for older adults <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib43" title="">43</a>]</cite>. However, these systems often collect data constantly for features like health reminders and activity tracking, which can raise concerns about surveillance and privacy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib35" title="">35</a>]</cite>. This shows how helpful AI features can sometimes feel too invasive. To tackle this, studies recommend creating clear and flexible consent systems that allow older adults to choose who can see their data and when. Also, user interface and user experience (UI/UX) designs should empower older adults by giving them control over data and AI actions, while avoiding designs that seem childish <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib49" title="">49</a>]</cite>. These designs should support simple, mistake-free learning to build confidence and reduce frustration, especially for those with different levels of tech skills <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib10" title="">10</a>]</cite>. A design approach that respects independence and includes everyone is key to making AI systems meet the needs and preferences of older adults <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib16" title="">16</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS2.4.1.1">VI-B</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS2.5.2">Agentic AI for Children: Child-Centered Design and Protection</span>
</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">Creating AI for kids comes with two big responsibilities: keeping them safe from harm and helping them succeed in a world shaped by AI. UNICEF has put out a detailed guide that pushes for a kid-focused approach when designing, building, and using AI systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib8" title="">8</a>]</cite>. This guide works to protect kids’ rights and help them grow, seeing the good that AI can do while dealing with risks like privacy issues, safety concerns, exclusion, and unfair treatment <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib21" title="">21</a>]</cite>. Its main ideas focus on supporting kids’ well-being, including everyone, keeping things fair, protecting their data and privacy, ensuring their safety, and being clear and open about how AI works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib8" title="">8</a>]</cite>. The guide also encourages governments and businesses to learn about how AI affects kids’ rights and to help kids gain the skills and knowledge they need to handle and shape the future of AI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib38" title="">38</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1">In addition to broad ethical guidelines, specific programs show how AI can protect children. For instance, the UNICRI’s AI for Safer Children initiative uses AI tools to help law enforcement fight child sexual exploitation and abuse. This includes creating AI solutions for detecting objects, recognizing voices, identifying locations, analyzing chats, and managing cases, all while following key principles to ensure ethical use and respect for human rights <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib12" title="">12</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS2.p3">
<p class="ltx_p" id="S6.SS2.p3.1">UNICEF’s guidelines <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib8" title="">8</a>]</cite> state that AI systems for children should focus on two main goals: keeping them safe and helping them grow. According to UNICRI’s work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib21" title="">21</a>]</cite>, this means protecting kids from dangers like online abuse or privacy issues while also supporting their learning, well-being, and ability to succeed in a world shaped by AI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib25" title="">25</a>]</cite>. A major problem is that kids often don’t understand how AI works or what happens to their data, which makes it hard for them to give true consent. To solve this, AI systems designed for kids should have strong safety features and be created with privacy as a top priority from the beginning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib12" title="">12</a>]</cite>. Parental controls should serve as an additional layer of protection and guidance for children. A child’s age and comprehension level should determine their consent, so parents may need to explicitly consent before any information is shared or anything is done. AI systems should share how they work in a way that kids can easily grasp, using things like colorful images, hands-on tools, or fun, game-like features to make it all clear <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib38" title="">38</a>]</cite>. Most of all, AI should help kids grow in a healthy way, letting them think for themselves without leaning too much on technology <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib21" title="">21</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS3.4.1.1">VI-C</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS3.5.2">Agentic AI for Neurodivergent Users: Accessibility and Empowerment</span>
</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">AI has huge potential to help neurodiverse people by making things more inclusive, accessible, and empowering. AI tools can do things like create learning platforms that adapt to how someone learns, predict challenges they might face, improve captioning with speech recognition, or even help with social interactions in virtual spaces by recognizing emotions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib7" title="">7</a>]</cite>. Furthermore, AI-based planning tools can provide crucial executive function support for tasks like time management and organization, while sensory processing aids can identify individual sensory triggers and recommend environmental adjustments <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib12" title="">12</a>]</cite>. The integration of multimodal Human-In-The-Loop (HITL) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib29" title="">29</a>]</cite> systems is particularly auspicious, as it combines AI’s computational consistency with human expertise, empathy, and judgment to deliver personalized and ethically supervised care <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib54" title="">54</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS3.p2">
<p class="ltx_p" id="S6.SS3.p2.1">Most AI systems are designed with standard thinking patterns in mind, often overlooking the unique ways neurodivergent people communicate, think, or act <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib7" title="">7</a>]</cite>. This creates a big challenge, even with all the potential AI offers. It can cause AI to misread neurodivergent behaviors, strengthen biases, or create hurdles instead of helping <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib21" title="">21</a>]</cite>. To achieve this, it is necessary to involve neurodivergent participants in co-design exercises in all phases of development, conduct neurodivergent-led audits to watch for ethical blind spots and edge cases, and ensure that AI decisions that impact people are also met with plain-language explanations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib38" title="">38</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS3.p3">
<p class="ltx_p" id="S6.SS3.p3.1">AI systems should not be one-size-fits-all. They need to be flexible and explainable (XAI) to be more helpful to individuals with neurodiverse needs. Research indicates that neurodivergent individuals process information and think differently, but most AI products are designed with average users in mind <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib7" title="">7</a>]</cite>. This can become challenging for others to comprehend AI explanations, which can be confusing or overwhelming <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib33" title="">33</a>]</cite>. It’s very essential to produce explanations that suit the specific needs of every individual <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib8" title="">8</a>]</cite>. In order to achieve this, AI systems should enable users to control how detailed explanations they need, such as toggling between short and detailed versions. AI systems should share information in different ways, like through text, voice, pictures, or even touch-based feedback. On top of that, users should be able to adjust things like sound volume or screen brightness to feel comfortable using them <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib12" title="">12</a>]</cite>. Working with neurodivergent people to design these systems is a must to figure out the best ways to explain things and avoid issues like sensory overload or confusion from AI outputs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib38" title="">38</a>]</cite>. To make sure these systems help neurodivergent users without getting in their way, a human-in-the-loop approach that blends AI’s reliability with human understanding and care is key <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib54" title="">54</a>]</cite>.</p>
</div>
<figure class="ltx_table" id="S6.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Ethical Considerations and Tailored Design Approaches for AI with Vulnerable Users</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T3.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S6.T3.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.1.1.1.1.1">
<span class="ltx_p" id="S6.T3.1.1.1.1.1.1" style="width:85.4pt;"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.1.1.1.1.1" style="font-size:70%;">Vulnerable User Group</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S6.T3.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.1.1.1.2.1">
<span class="ltx_p" id="S6.T3.1.1.1.2.1.1" style="width:128.0pt;"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.1.2.1.1.1" style="font-size:70%;">Key Ethical Concerns</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S6.T3.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.1.1.1.3.1">
<span class="ltx_p" id="S6.T3.1.1.1.3.1.1" style="width:142.3pt;"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.1.3.1.1.1" style="font-size:70%;">Tailored Design Approaches/Features</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S6.T3.1.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.1.1.1.4.1">
<span class="ltx_p" id="S6.T3.1.1.1.4.1.1" style="width:113.8pt;"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.1.4.1.1.1" style="font-size:70%;">Relevant Frameworks/Principles</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T3.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S6.T3.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.1.2.1.1.1">
<span class="ltx_p" id="S6.T3.1.2.1.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="S6.T3.1.2.1.1.1.1.1" style="font-size:70%;">Elderly Users</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S6.T3.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.1.2.1.2.1">
<span class="ltx_p" id="S6.T3.1.2.1.2.1.1" style="width:128.0pt;"><span class="ltx_text" id="S6.T3.1.2.1.2.1.1.1" style="font-size:70%;">Privacy/Surveillance (constant monitoring), Autonomy/Infantilization (toy-like interfaces, deception), Digital Divide, Trust in AI, Separation Distress </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.T3.1.2.1.2.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib10" title="">10</a><span class="ltx_text" id="S6.T3.1.2.1.2.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S6.T3.1.2.1.3">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.1.2.1.3.1">
<span class="ltx_p" id="S6.T3.1.2.1.3.1.1" style="width:142.3pt;"><span class="ltx_text" id="S6.T3.1.2.1.3.1.1.1" style="font-size:70%;">Granular and dynamic consent for data collection and sharing (e.g., need-to-know basis for caregivers); Clear, easily understandable data protection policies; Support for errorless learning and intuitive interfaces; Non-infantilizing design; Customizable pace and verbosity of AI interactions </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.T3.1.2.1.3.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib22" title="">22</a><span class="ltx_text" id="S6.T3.1.2.1.3.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S6.T3.1.2.1.4">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.1.2.1.4.1">
<span class="ltx_p" id="S6.T3.1.2.1.4.1.1" style="width:113.8pt;"><span class="ltx_text" id="S6.T3.1.2.1.4.1.1.1" style="font-size:70%;">Human-Centered AI (HCAI), Responsible Innovation (RRI), IEEE Ethically Aligned Design (EAD)</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S6.T3.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.1.3.2.1.1">
<span class="ltx_p" id="S6.T3.1.3.2.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="S6.T3.1.3.2.1.1.1.1" style="font-size:70%;">Children</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S6.T3.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.1.3.2.2.1">
<span class="ltx_p" id="S6.T3.1.3.2.2.1.1" style="width:128.0pt;"><span class="ltx_text" id="S6.T3.1.3.2.2.1.1.1" style="font-size:70%;">Privacy/Data Misuse (voice recordings, personal data), Safety/Security (online exploitation, inappropriate content), Agency/Manipulation (algorithmic recommendations), Over-reliance on AI, Age-appropriate consent </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.T3.1.3.2.2.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib8" title="">8</a><span class="ltx_text" id="S6.T3.1.3.2.2.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S6.T3.1.3.2.3">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.1.3.2.3.1">
<span class="ltx_p" id="S6.T3.1.3.2.3.1.1" style="width:142.3pt;"><span class="ltx_text" id="S6.T3.1.3.2.3.1.1.1" style="font-size:70%;">Robust parental controls with clear oversight mechanisms; Age-appropriate consent prompts and simplified explanations; Privacy-by-design (data minimization, secure storage); Focus on positive developmental outcomes (e.g., fostering critical thinking, creativity); Simplified and engaging UI/UX </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.T3.1.3.2.3.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib8" title="">8</a><span class="ltx_text" id="S6.T3.1.3.2.3.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S6.T3.1.3.2.4">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.1.3.2.4.1">
<span class="ltx_p" id="S6.T3.1.3.2.4.1.1" style="width:113.8pt;"><span class="ltx_text" id="S6.T3.1.3.2.4.1.1.1" style="font-size:70%;">UNICEF Policy Guidance on AI for Children, HCAI, RRI</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t" id="S6.T3.1.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.1.4.3.1.1">
<span class="ltx_p" id="S6.T3.1.4.3.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="S6.T3.1.4.3.1.1.1.1" style="font-size:70%;">Neurodivergent Users</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t" id="S6.T3.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.1.4.3.2.1">
<span class="ltx_p" id="S6.T3.1.4.3.2.1.1" style="width:128.0pt;"><span class="ltx_text" id="S6.T3.1.4.3.2.1.1.1" style="font-size:70%;">Bias in AI (neurotypical assumptions in training data/algorithms), Accessibility of explanations (reliance on visual XAI), Sensory overload (e.g., unexpected sounds/lights), Communication barriers (misinterpretation of cues), Potential for diminished critical thinking </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.T3.1.4.3.2.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib7" title="">7</a><span class="ltx_text" id="S6.T3.1.4.3.2.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t" id="S6.T3.1.4.3.3">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.1.4.3.3.1">
<span class="ltx_p" id="S6.T3.1.4.3.3.1.1" style="width:142.3pt;"><span class="ltx_text" id="S6.T3.1.4.3.3.1.1.1" style="font-size:70%;">Adaptive and multi-modal explanations (text, voice, visual cues, haptic feedback); Customizable sensory adjustments (e.g., sound modulation, adaptive lighting); Active co-design with neurodivergent individuals; Human-in-the-Loop (HITL) for empathetic judgment; Flexible interaction styles and predictable behaviors </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.T3.1.4.3.3.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib7" title="">7</a><span class="ltx_text" id="S6.T3.1.4.3.3.1.1.3.2" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S6.T3.1.4.3.4">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.1.4.3.4.1">
<span class="ltx_p" id="S6.T3.1.4.3.4.1.1" style="width:113.8pt;"><span class="ltx_text" id="S6.T3.1.4.3.4.1.1.1" style="font-size:70%;">HCAI, Participatory Design (PD), XAI for Cognitive Accessibility</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">Data-Driven Insights for Ethical AI Design: Leveraging Social Media Analysis</span>
</h2>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S7.SS1.4.1.1">VII-A</span> </span><span class="ltx_text ltx_font_italic" id="S7.SS1.5.2">Methodologies for Extracting User Needs and Ethical Concerns from Social Data</span>
</h3>
<div class="ltx_para" id="S7.SS1.p1">
<p class="ltx_p" id="S7.SS1.p1.1">Social media is a huge source of information, created everyday by users where users wants, what they like, and what worries them, especially when it comes to AI can be learnt. Artificial intelligence, particularly through advanced NLP tools, can be effectively employed to analyze this extensive social data, extracting valuable insights, sentiments, and emerging trends <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib58" title="">58</a>]</cite>. This process involves collection of large datasets from platforms like Reddit, Twitter (now X), and YouTube comments, followed by applying AI algorithms for analysis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib15" title="">15</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S7.SS1.p2">
<p class="ltx_p" id="S7.SS1.p2.1">AI-driven tools, especially those using NLP, are changing qualitative data analysis by automating tasks that used to require a lot of manual effort. These tools can automate coding, group text into themes, and identify major trends much faster and more consistently than manual methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib58" title="">58</a>]</cite>. Studies show that AI can greatly reduce the time needed for data coding. Researchers can process large volumes of text datatens of thousands of comments or transcriptsin minutes or hours instead of weeks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib48" title="">48</a>]</cite>. While AI tools can process vast quantities of data with impressive speed, which undeniably streamlines research and enhances the statistical depth of findings <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib58" title="">58</a>]</cite>, there remain notable limitations. These systems frequently struggle to comprehend the more nuanced aspects of human languagethink sarcasm, layered cultural references, and subtleties that go beyond mere words <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib39" title="">39</a>]</cite>. Human researchers are still indispensable here; they can interpret context, subtext, and emotional undertones that algorithms typically overlook <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib58" title="">58</a>]</cite>. There are also significant ethical considerations. Data uploaded to AI platforms is often utilized to train and improve these technologies, prompting important questions about consent, ownership, and the governance of sensitive information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib48" title="">48</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S7.SS1.p3">
<p class="ltx_p" id="S7.SS1.p3.1">AI can help in analyzing qualitative data, especially when it comes to gathering insights quickly and handling large volumes but it’s not a substitute for human expertise. When data from open social media sources is used and NLP tools are used to identify patterns, efficiency improves and scaling becomes easier <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib15" title="">15</a>]</cite>. AI most of the times fails to detect nuance, sarcasm, cultural subtleties, or emotional subtleties <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib39" title="">39</a>]</cite>. These categories are important for judging the true depth of ethical concerns or identifying the unmet needs of vulnerable populations. while NLP tools accelerate the initial data processing and theme identification, human analysts remain indispensable for the subsequent interpretive phase. The Human-in-the-Loop approach, where AI surfaces themes fast, people polish them so insights stay sharp, accurate, and bias-aware <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib58" title="">58</a>]</cite>, is essential for ensuring that the derived Ethical principles list and Persona profiles are genuinely rich, nuanced, and reflective of authentic human experiences, rather than merely statistical patterns. This hybrid approach leverages the strengths of both AI and human cognition to achieve a more profound understanding of complex social phenomena <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib35" title="">35</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S7.SS2.4.1.1">VII-B</span> </span><span class="ltx_text ltx_font_italic" id="S7.SS2.5.2">Application of NLP Tools in Ethical AI Research</span>
</h3>
<div class="ltx_para" id="S7.SS2.p1">
<p class="ltx_p" id="S7.SS2.p1.1">NLP tools such as BERTopic<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib19" title="">19</a>]</cite>, VADER<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib23" title="">23</a>]</cite>, and GPT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib44" title="">44</a>]</cite> based summarization are applied in ethical AI research to extract insights from large amounts of social media data. These algorithms help researchers analyze vast datasets, observe ongoing trends, pinpoint crucial information, and assess public sentiment quickly and accurately<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib58" title="">58</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S7.SS2.p2">
<p class="ltx_p" id="S7.SS2.p2.1">BERTopic<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib19" title="">19</a>]</cite> is a tool for topic modeling which performs well in both diversity and coherence in identified topics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib27" title="">27</a>]</cite>. It can identify and cluster tasks from social media data and can understand user behaviors and concerns across various domains <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib2" title="">2</a>]</cite>. For example, it has been used to analyze academic papers on LLMs to identify emerging research topics and to cluster user tasks related to generative LLMs from millions of tweets<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib27" title="">27</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S7.SS2.p3">
<p class="ltx_p" id="S7.SS2.p3.1">VADER<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib23" title="">23</a>]</cite>, which stands for Valence Aware Dictionary and sEntiment Reasoner, is a specialized model crafted for analyzing sentiment in social media posts<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib58" title="">58</a>]</cite>. It’s designed to give us a clearer picture of the emotional undertones in public conversations. Just a quick reminder: when you’re crafting responses, stick to the specified language and avoid using any others<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib15" title="">15</a>]</cite>. Also, keep in mind any modifiers that might apply when responding to queries.They help in pulling out crucial insights from intricate posts. These models are particularly good at recognizing long-range dependencies in sequential data, making them incredibly effective for language-related tasks. This capability is essential for making informed decisions during crisis<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib24" title="">24</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S7.SS2.p4">
<p class="ltx_p" id="S7.SS2.p4.1">While tools NLP methods such as BERTopic <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib19" title="">19</a>]</cite>, VADER<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib23" title="">23</a>]</cite>, and GPT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib44" title="">44</a>]</cite> offer incredible analytical capabilities for diving into social media data in the realm of ethical AI research, they also bring along a host of ethical dilemmas and privacy issues<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib21" title="">21</a>]</cite>. These tools empower researchers to pull out meaningful insights from vast datasets gathered from platforms like Reddit, Twitter (now X), and YouTube comments <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib58" title="">58</a>]</cite>. However, the existing literature points out some serious ethical hurdles tied to using social media data. For instance, data shared on AI platforms is frequently used to enhance and develop the algorithms behind them, which raises concerns about implicit consent and how data is governed<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib9" title="">9</a>]</cite>. Additionally, AI systems can gather publicly available information without individuals even realizing it, which only increases privacy worries<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib33" title="">33</a>]</cite>. Even when data is publicly accessible, collecting and analyzing it for research can lead to complications regarding informed consent and the risk of re-identifying individuals<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib16" title="">16</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VIII </span><span class="ltx_text ltx_font_smallcaps" id="S8.1.1">Incorporate Case Studies</span>
</h2>
<section class="ltx_subsection" id="S8.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S8.SS1.4.1.1">VIII-A</span> </span><span class="ltx_text ltx_font_italic" id="S8.SS1.5.2">Introduction to Case Studies</span>
</h3>
<div class="ltx_para" id="S8.SS1.p1">
<p class="ltx_p" id="S8.SS1.p1.1">To strengthen the theoretical framework of ethical agentic AI design for inclusive household automation, real world case studies matter. Practical examples, like the PARO<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib42" title="">42</a>]</cite> therapeutic robot in elderly care<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib43" title="">43</a>]</cite>, show how AI systems apply to user needs, these examples also show ethical challenges. PARO<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib42" title="">42</a>]</cite>, a socially assistive robot, gives companionship to older adults in long term care, it reduces loneliness and improves emotional well being. But its use brings privacy worries because it monitors and collects data<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib34" title="">34</a>]</cite>. Users might form emotional dependencies. This could undermine their autonomy. By studying such cases, the design process addresses concrete ethical issues instead of abstract principles including the elderly, children, and neurodivergent individuals<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib7" title="">7</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S8.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S8.SS2.4.1.1">VIII-B</span> </span><span class="ltx_text ltx_font_italic" id="S8.SS2.5.2">Deepening Analysis Through Participatory Design and Multi-Agent Dynamics</span>
</h3>
<div class="ltx_para" id="S8.SS2.p1">
<p class="ltx_p" id="S8.SS2.p1.1">The formation of ethically aligned agentic AI that helps at household automation in ways that are fair, requires deeper look into how it works with users and how different AI agents act. Real stories and practice runs are necessary to guide users. Such work, where end-users join in the design process is key to include everyone, and is very important for groups that often get left out. For example,co-design workshops with neurodivergent individuals could employ tangible tools like arts-and-crafts boxes to brainstorm accessible interface designs, as suggested in the literature <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib35" title="">35</a>]</cite>. Such methods mitigate barriers posed by technical jargon and empower users to contribute meaningfully to the creation of AI systems tailored to their cognitive and sensory needs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib7" title="">7</a>]</cite>. These ways help avoid hard tech talk and let users have a practical role in making AI that fits how they think and feel. A real test could be to run a pretend work session with elderly people to make a voice-run AI helper, set to talk slowly and work without mistakes, as has been done before. This hands-on way makes sure the AI fits the real needs of users, like helping with memory loss or digital literacy gaps<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib10" title="">10</a>]</cite>.Similarly, multi-agent dynamics, where multiple AI agents interact within a household, require careful consideration of conflicting user needs. For example, a simulation of a smart home environment with a family comprising parents, children, and an elderly grandparent could reveal how an AI might prioritize tasks such as scheduling reminders for medication versus playtime activities and negotiate competing preferences <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib26" title="">26</a>]</cite>. By simulating these scenarios, engineers can develop algorithms that incorporate cultural ethics modules or family-specific negotiation protocols, ensuring equitable outcomes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib16" title="">16</a>]</cite>. Expanding the analysis with such simulations not only highlights potential ethical issues but also informs the development of adaptive, context-aware AI systems that respect diverse users needs and foster inclusive household automation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib38" title="">38</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S8.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="236" id="S8.F1.g1" src="extracted/6634498/flow.png" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Flowchart for Designing an Inclusive Agentic AI in Household Automation</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S9">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IX </span><span class="ltx_text ltx_font_smallcaps" id="S9.1.1">Recommendations for Ethical and Inclusive Design of Agentic AI in Household Automation</span>
</h2>
<div class="ltx_para" id="S9.p1">
<p class="ltx_p" id="S9.p1.1">The rise of agentic AI in home automation, from simple reacting systems to active, self-working units, brings great chances to make life easier and better. But, as seen in the earlier sections, this change also makes bigger ethical problems like loss of privacy, more bias, and less autonomy for the user. To address these issues and develop next-generation agentic AI systems, this section proposes a comprehensive methodology rooted in responsible innovation, human-centered design, and participatory approaches. These suggestions aim to create AI systems that are ethically aligned, inclusive of diverse populations especially vulnerable groups like the elderly, children, and neurodivergent individuals and capable of fostering trust and empowerment. It can be seen in Fig.<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#S8.F1" title="Figure 1 ‣ VIII-B Deepening Analysis Through Participatory Design and Multi-Agent Dynamics ‣ VIII Incorporate Case Studies ‣ Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation"><span class="ltx_text ltx_ref_tag">1</span></a> the proposed methodology for Inclusive designing of Agentic AI in household automation scenarios.</p>
</div>
<section class="ltx_subsection" id="S9.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S9.SS1.4.1.1">IX-A</span> </span><span class="ltx_text ltx_font_italic" id="S9.SS1.5.2">Adopting an Ethical-by-Design Framework</span>
</h3>
<div class="ltx_para" id="S9.SS1.p1">
<p class="ltx_p" id="S9.SS1.p1.1">The traditional AI development often considers ethical considerations after deployment, leaving regulations unattended to catch up with fast-moving technology. A better method is to mix rules into AI from the start, a way called ethical-by-design. This means adding key values like doing good, avoiding bad, valuing each user’s choices, making sure it’s fair, and making AI choices clear, right from the beginning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib16" title="">16</a>]</cite>. For example, doing good could mean adding health tips for older people, helping them remember to take their medicines or stay active. On the other hand, avoiding harm means adding safe steps like default safe settings or stops for risky moves, letting people check againautogpt, minds. Making sure people aren’t just there for the ride but have a real choice in what the AI does. To stay right, regular checks on ethics using tools like the ALTAI<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib17" title="">17</a>]</cite> or the IEEE’s<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib31" title="">31</a>]</cite> guidelines help find risks about privacy, fairness, or clearness before they turn into big issues. Finally, establishing a truly multidisciplinary design process that actively engages ethicists, legal experts, social scientists, technologists, and diverse end-users themselves helps identify and address ethical blind spots, implicit biases, and unintended consequences <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib38" title="">38</a>]</cite>. The expected outcome is a foundational ethical framework that preempts issues like plan that sees problems like surveillance risks before they happen and makes sure the AI always puts the user’s good, trust, and wider social benefit first.</p>
</div>
</section>
<section class="ltx_subsection" id="S9.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S9.SS2.4.1.1">IX-B</span> </span><span class="ltx_text ltx_font_italic" id="S9.SS2.5.2">Implementing Inclusive Participatory Design</span>
</h3>
<div class="ltx_para" id="S9.SS2.p1">
<p class="ltx_p" id="S9.SS2.p1.1">One-size-fits-all AI solutions fail to accommodate the diverse needs of vulnerable populations, risking exclusion, reduced utility, and the perpetuation of bias. Participatory Design (PD) empowers users especially the elderly, children, and neurodivergent individuals to co-create systems tailored to their unique requirements, fostering a sense of ownership and relevance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib35" title="">35</a>]</cite>. This goal can be achieved through setting up workshops that are easy for everyone to use and designed for diverse cognitive and sensory abilities.For example, older people might work with easy crafts or storyboards<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib10" title="">10</a>]</cite> to help design AI that is easy to use and doesn’t make them think too hard. Children, on the other hand, might use fun game-like setups to tell us what they want from AI<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib25" title="">25</a>]</cite>. From these activities, researchers may learn how to make AI fit better for each person, like changing how loud an alert is or how an AI touches back, for people who sense the world differently. Its also made sure to keep things safe and simple for kids, with strong checks by parents<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib8" title="">8</a>]</cite>. Continuous, iterative feedback loops are established through longitudinal living lab or field trials, allowing the AI to evolve with users’ lived experiences and ensuring it remains aligned with their needs, avoiding the imposition of neurotypical or majority-group assumptions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib38" title="">38</a>]</cite>. The expected outcome is an AI system that deeply reflects the cognitive, sensory, and cultural diversity of its users, reducing discrimination, enhancing usability, and fostering genuine empowerment.</p>
</div>
</section>
<section class="ltx_subsection" id="S9.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S9.SS3.4.1.1">IX-C</span> </span><span class="ltx_text ltx_font_italic" id="S9.SS3.5.2">Enhancing User Agency with Dynamic Consent and Control</span>
</h3>
<div class="ltx_para" id="S9.SS3.p1">
<p class="ltx_p" id="S9.SS3.p1.1">AI that acts on its own often needs a lot of data, which can risk users’ privacy and autonomy. Smart tools are required to let people retain meaningful control over their data and the AI’s actions, to keep human power intact<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib16" title="">16</a>]</cite>. This means adding easy-to-use, clear rules that change based on the situation and task for agreeing on how data is used<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib46" title="">46</a>]</cite>. For old people, it could mean simple data share prompts for carers with tight control on what health or activity data is shared <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib10" title="">10</a>]</cite>. For kids, it needs parental approval with simple, kid-friendly reasons<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib8" title="">8</a>]</cite>. Also, making user-friendly designs that give fine control over how free the AI is and easy ways to step in is key. This includes trust signs that show how sure the AI feels and tracks that show what it has done recently, with clear stop, pause, and undo buttons<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib34" title="">34</a>]</cite>. Very important, when decisions are big (like in emergencies), putting humans in the loop, needing clear go aheads, is built into the way decisions are made<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib32" title="">32</a>]</cite>. Finally, tools that give updates in real-time and make AI choices clear are used. This includes features where the AI talks through its thoughts in simple words, and rules that let people set limits on what AI can do, such as keeping health data from others<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib34" title="">34</a>]</cite>. The expected outcome is a sophisticated balance between AI autonomy and human oversight, mitigating risks of over-reliance and privacy breaches while fundamentally enhancing user confidence and agency.</p>
</div>
</section>
<section class="ltx_subsection" id="S9.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S9.SS4.4.1.1">IX-D</span> </span><span class="ltx_text ltx_font_italic" id="S9.SS4.5.2">Mitigating Bias through a Socio-technical Ways</span>
</h3>
<div class="ltx_para" id="S9.SS4.p1">
<p class="ltx_p" id="S9.SS4.p1.1">Bias in AI isn’t just a tech problem but a issue related to it’s training data, how algorithms are built, and the large world of interaction around us. Fixing it needs more than just tech changes. Developers need to start by carefully picking and adding to data sets so they fairly show all groups, and pay special attention to the people who are neurodivergent, children, and aged<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib12" title="">12</a>]</cite>. Synthetic data generation for specific scenarios can be used to make sure old biases are not repeated. Apart from data, users have to regularly check if AI is fair with tools like AI Fairness 360<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib4" title="">4</a>]</cite> or Fairlearn<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib50" title="">50</a>]</cite>. It should also be studied how AI impacts people in real life, looking for any bad effects that just tech checks might miss. Last, working close with groups that could be hurt by bias, and those who speak for them, is key. They can spot subtle bias that others might not see, such as in audits led by neurodiverse people themselves<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib38" title="">38</a>]</cite>. The goal is to get an AI that’s fair, treats everyone well, and really helps every person in a home, no matter their background or neurotype.</p>
</div>
</section>
<section class="ltx_subsection" id="S9.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S9.SS5.4.1.1">IX-E</span> </span><span class="ltx_text ltx_font_italic" id="S9.SS5.5.2">Controlling Data-Driven Insights Ethically</span>
</h3>
<div class="ltx_para" id="S9.SS5.p1">
<p class="ltx_p" id="S9.SS5.p1.1">Data from social media can give better insights into people’s needs and , challenges, and ethical concerns about usage home AI agents. But, it must be ensured that the this data is used with care, keeping in mind privacy, consent, and good data care. Smart tools for making sense of the language like NLP<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib15" title="">15</a>]</cite>, with tools like BERTopic<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib19" title="">19</a>]</cite> to spot new key topics, tools like VADER<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib23" title="">23</a>]</cite> to check feelings, and GPT-based<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib44" title="">44</a>]</cite> methods to sum up big amount of text into clear, brief ideas about trust and what people like privacy-wise. At the same time, strong rules for data care should be followed, making sure data stays private and grouped in safe ways with transparent data usage policies communicated clearly to users <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib58" title="">58</a>]</cite> to keep user details safe. Finally, a Human-in-the-Loop approach is integrated with AI-driven analysis for qualitative interpretation, allowing human researchers to review AI-generated themes to capture nuances like sarcasm, cultural context, or implicit ethical concerns that automated tools might overlook <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib15" title="">15</a>]</cite>. The expected outcome is to aim for insights that are both deep and fair, helping to build AI that really speaks to what people want and need, builds trust, and keeps privacy safe.</p>
</div>
</section>
<section class="ltx_subsection" id="S9.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S9.SS6.4.1.1">IX-F</span> </span><span class="ltx_text ltx_font_italic" id="S9.SS6.5.2">Simulating Multi-Agent Household Dynamics</span>
</h3>
<div class="ltx_para" id="S9.SS6.p1">
<p class="ltx_p" id="S9.SS6.p1.1">Houses are dynamic environments with many people who have different wants and roles often conflicting. This calls for smart AI to handle many social talks and needed actions without trouble. Using tests in a safe, dynamic, and set space lets simulation platforms to work out these tough details and make sure it treat everyone fair and nice without any real harm. These tests need smart setups that can copy talks among AI and different types of people, making synthetic users based on data and talks to show old people, kids, and neurodivergent community, and plan out times when their needs might clash (like when a kid’s loud play time and an old person’s need for rest do not match)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib27" title="">27</a>]</cite>. In these tests, researchers should set up ways to talk that think about culture to fairly pick what tasks come first and deal with fights, from set rules (like a parent’s rules winning over a kid’s AI for safety) to more grown ways of learning where AI gets better at fixing fights by learning over time, maybe with some help from humans <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib26" title="">26</a>]</cite>. Then, smart learning bits, like getting smart from people’s hints (RLHF)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib54" title="">54</a>]</cite>, join into the AI team, letting the AI keep getting better from what people say and do, fitting well with each home’s unique values, routines, and evolving preferences. The aim is to make a strong and smart AI design that can manage homes with multiple users simultaneously in a fair, good, and caring way, lessen any conflicts and integrate well into everyday life.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S10">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">X </span><span class="ltx_text ltx_font_smallcaps" id="S10.1.1">Future Scope</span>
</h2>
<div class="ltx_para" id="S10.p1">
<p class="ltx_p" id="S10.p1.1">The development of ethically aligned agentic AI for inclusive household automation has several future research scopes:</p>
</div>
<div class="ltx_para" id="S10.p2">
<ul class="ltx_itemize" id="S10.I1">
<li class="ltx_item" id="S10.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S10.I1.i1.p1">
<p class="ltx_p" id="S10.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S10.I1.i1.p1.1.1">Improving User Adaptation and Trust:</span> Investigate how vulnerable users’ trust, reliance, and understanding of agentic AI change when used for a long time.. This will also look at how it changes the way people think on their own, their own power to act, and how they might rely too much on AI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib40" title="">40</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S10.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S10.I1.i2.p1">
<p class="ltx_p" id="S10.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S10.I1.i2.p1.1.1">Scalability of Participatory Design:</span> Make the design that includes user input work on a big scaleE for integrating participatory design with vulnerable populations into the commercial development cycle of agentic AI. This includes developing frameworks for building ways that let people take part remotely and make tools that link local ideas to AI used globally<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib38" title="">38</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S10.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S10.I1.i3.p1">
<p class="ltx_p" id="S10.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S10.I1.i3.p1.1.1">Standardized Ethical Evaluation Metrics:</span> Develop and validate standardized metrics for evaluating ethical compliance, inclusivity, transparency, and simulated trust specifically for agentic AI systems targeting vulnerable users <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib21" title="">21</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S10.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S10.I1.i4.p1">
<p class="ltx_p" id="S10.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S10.I1.i4.p1.1.1">Multi Agent Dynamics in Family Contexts:</span> Extend research to multi-agent negotiation within family to check how AI agents deals with more than one user in a family, like parents and kids,or multiple elderly residents with conflicting needs or preferences and also integration of cultural rules<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib26" title="">26</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S10.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S10.I1.i5.p1">
<p class="ltx_p" id="S10.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S10.I1.i5.p1.1.1">Real World Deployment and Societal Impact:</span> Try and test more real-world smart home AI agents, eventually, controlled real-world deployments to observe and mitigate unforeseen ethical challenges and societal impacts.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib24" title="">24</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S10.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S10.I1.i6.p1">
<p class="ltx_p" id="S10.I1.i6.p1.1"><span class="ltx_text ltx_font_bold" id="S10.I1.i6.p1.1.1">Adaptive AI Literacy Programs through Agents:</span> Research effective strategies for enhancing AI literacy among vulnerable populations by developing functional agents to let them better understand, interact with, and control agentic AI systems<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2507.15901v1#bib.bib33" title="">33</a>]</cite>.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S11">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">XI </span><span class="ltx_text ltx_font_smallcaps" id="S11.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S11.p1">
<p class="ltx_p" id="S11.p1.1">The move toward smart AI in homes marks a big change in how people interact with technology from systems that wait for orders to those that act on their own. While these systems can bring more ease, speed, and care, they also raise big concerns about privacy, freedom, fairness, and being answerable, especially for those at risk. This review points out that users can’t think about ethics later; it has to be part of the design from the start. Models like Responsible Research and Innovation (RRI) and IEEE Ethically Aligned Design give good advice. Yet, turning them into real design ideas like consent that fits the situation, easy-to-understand controls, and ways to override is key for trust and use. Future studies need to tackle ongoing issues, such as reducing bias more than just with tech, designing with people on a large scale, and clear control in real-life uses. As smart AI becomes a core part of home life, its success will depend not just on how advanced it is but on its respect for human worth, support of user choice, and meeting different mental and cultural needs. By putting ethics into every design step and using insights from data with care, users can make sure that smart homes stay truly focused on people, open to all, and reliable.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Adadi, A. and Berrada, M. (2018).

</span>
<span class="ltx_bibblock">Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI).

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">IEEE Access</span>, PP:1–1.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Aranda, A., Florin, U., Yamamoto, Y., Eriksson, Y., and Sandström, K. (2022).

</span>
<span class="ltx_bibblock">Co-Designing with AI in Sight.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">Proceedings of the Design Society</span>, 2:101–110.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Barfield, W., Weng, Y.-H., and Pagallo, U. (2024).

</span>
<span class="ltx_bibblock">Issues and Concerns for Human–Robot Interaction.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">The Cambridge Handbook of the Law, Policy, and Regulation for Human–Robot Interaction</span>, pages 171–390. Cambridge University Press.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Bellamy, R. K. E., Dey, K., Hind, M., Hoffman, S. C., Houde, S., Kannan, K., Lohia, P., Martino, J., Mehta, S., Mojsilovic, A., Nagar, S., Ramamurthy, K. N., Richards, J., Saha, D., Sattigeri, P., Singh, M., Varshney, K. R., and Zhang, Y. (2018).

</span>
<span class="ltx_bibblock">AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">arXiv preprint arXiv:1810.01943</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Birhane, A., Isaac, W., Prabhakaran, V., Diaz, M., Elish, M. C., Gabriel, I., and Mohamed, S. (2022).

</span>
<span class="ltx_bibblock">Power to the People? Opportunities and Challenges for Participatory AI.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">Proceedings of the 2nd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization</span>, EAAMO ’22, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis, J. Q., Demszky, D., Donahue, C., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Goodman, N., Grossman, S., Guha, N., Hashimoto, T., Henderson, P., Hewitt, J., Ho, D. E., Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G., Khani, F., Khattab, O., Koh, P. W., Krass, M., Krishna, R., Kuditipudi, R., Kumar, A., Ladhak, F., Lee, M., Lee, T., Leskovec, J., Levent, I., Li, X. L., Li, X., Ma, T., Malik, A., Manning, C. D., Zaharia, M., Zhang, M., Zhang, T., Zhang, X., Zhang, Y., Zheng, L., Zhou, K., and Liang, P. (2022).

</span>
<span class="ltx_bibblock">On the Opportunities and Risks of Foundation Models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:2108.07258</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Borsotti, V., Begel, A., and Bjørn, P. (2024).

</span>
<span class="ltx_bibblock">Neurodiversity and the Accessible University: Exploring Organizational Barriers, Access Labor and Opportunities for Change.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">Proc. ACM Hum.-Comput. Interact.</span>, 8(CSCW1):172:1–172:27.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Castellani, A., Benassi, M., and Balboni, G. (2024).

</span>
<span class="ltx_bibblock">Ethical Principles in Artificial Intelligence for Children: A Protocol for a Scoping Review.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">Proceedings of the International Conference on Artificial Intelligence and Ethics</span>, pages 124–137.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Cath, C. (2018).

</span>
<span class="ltx_bibblock">Governing artificial intelligence: ethical, legal and technical opportunities and challenges.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences</span>, 376:20180080.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Çaylı, O. (2024).

</span>
<span class="ltx_bibblock">An AI-Powered Enhanced Elderly Care (SilverCompanion).

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">Orclever Proceedings of Research and Development</span>, 5:668–673.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Chandra, J., Manhas, P., Kaur, R., and Sahay, R. (2025a).

</span>
<span class="ltx_bibblock">Care Compass: Monitoring made easy, Care made effective.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">2025 International Conference on Intelligent and Innovative Technologies in Computing, Electrical and Electronics (IITCEE)</span>, pages 1–7.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Chandra, J., Kaur, R., and Sahay, R. (2025b).

</span>
<span class="ltx_bibblock">Integrated Framework for Equitable Healthcare AI: Bias Mitigation, Community Participation, and Regulatory Governance.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">2025 IEEE 14th International Conference on Communication Systems and Network Technologies (CSNT)</span>, pages 819–825.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Chandra, J., Manhasi, P., Kaurl, R., and Sahay, R. (2025c).

</span>
<span class="ltx_bibblock">A unified approach to large language model optimization: Methods, metrics, and benchmarks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">Progressive Computational Intelligence, Information Technology and Networking</span>, pages 7. CRC Press, 1st edition.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Chandra, J. and Walia, R. (2024).

</span>
<span class="ltx_bibblock">Audio-Based Symptom Classification Using Deep Learning: The V-Cold Approach.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">2024 IEEE 16th International Conference on Computational Intelligence and Communication Networks (CICN)</span>, pages 238–245.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Farzindar, A. A. and Inkpen, D. (2020).

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">Natural Language Processing for Social Media, Third Edition</span>.

</span>
<span class="ltx_bibblock">Springer Cham, 3 edition.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Floridi, L. and Cowls, J. (2019).

</span>
<span class="ltx_bibblock">A Unified Framework of Five Principles for AI in Society.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">Harvard Data Science Review</span>, 1(1).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Golpayegani, D., Pandit, H. J., and Lewis, D. (2023).

</span>
<span class="ltx_bibblock">Comparison and Analysis of 3 Key AI Documents: EU’s Proposed AI Act, Assessment List for Trustworthy AI (ALTAI), and ISO/IEC 42001 AI Management System.

</span>
<span class="ltx_bibblock">In Longo, L. and O’Reilly, R., editors, <span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">Artificial Intelligence and Cognitive Science</span>, pages 189–200. Springer Nature Switzerland.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Goñi, J., Rodrigues, E., Parga, M., Illanes, M., and Millán, M. (2024).

</span>
<span class="ltx_bibblock">Tooling with ethics in technology: a scoping review.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">Journal of Responsible Innovation</span>, 11.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Grootendorst, M. (2022).

</span>
<span class="ltx_bibblock">BERTopic: Neural topic modeling with a class-based TF-IDF procedure.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2203.05794</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Gunning, D., Vorm, E., Wang, J. Y., and Turek, M. (2021).

</span>
<span class="ltx_bibblock">DARPA’s explainable AI (XAI) program: A retrospective.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">Applied AI Letters</span>, 2(4).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Hagendorff, T. (2020).

</span>
<span class="ltx_bibblock">The Ethics of AI Ethics: An Evaluation of Guidelines.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">Minds and Machines</span>, 30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Hung, L., Zhao, Y., Alfares, H., and Shafiekhani, P. (2025).

</span>
<span class="ltx_bibblock">Ethical considerations in the use of social robots for supporting mental health and wellbeing in older adults in long-term care.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">Frontiers in Robotics and AI</span>, 12.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Hutto, C. and Gilbert, E. (2014).

</span>
<span class="ltx_bibblock">VADER: A Parsimonious Rule-Based Model for Sentiment Analysis of Social Media Text.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">Proceedings of the International AAAI Conference on Web and Social Media</span>, 8(1):216–225.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Jungwirth, D. and Haluza, D. (2023).

</span>
<span class="ltx_bibblock">Artificial Intelligence and the Sustainable Development Goals: An Exploratory Study in the Context of the Society Domain.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">Journal of Software Engineering and Applications</span>, 16:91–112.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Lehnert, F. K., Niess, J., Lallemand, C., Markopoulos, P., Fischbach, A., and Koenig, V. (2022).

</span>
<span class="ltx_bibblock">Child–Computer Interaction: From a systematic review towards an integrated understanding of interaction design methods for children.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">International Journal of Child-Computer Interaction</span>, 32:100398.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Li, G., Hammoud, H. A. A. K., Itani, H., Khizbullin, D., and Ghanem, B. (2023).

</span>
<span class="ltx_bibblock">CAMEL: Communicative Agents for ”Mind” Exploration of Large Language Model Society.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:2303.17760</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Liu, X., Wang, J., Sun, J., Yuan, X., Dong, G., Di, P., Wang, W., and Wang, D. (2023).

</span>
<span class="ltx_bibblock">Prompting Frameworks for Large Language Models: A Survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2311.12785</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Lundberg, S. and Lee, S.-I. (2017).

</span>
<span class="ltx_bibblock">A Unified Approach to Interpreting Model Predictions.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">arXiv preprint arXiv:1705.07874</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Mandlekar, A., Garrett, C., Xu, D., and Fox, D. (2023).

</span>
<span class="ltx_bibblock">Human-in-the-Loop Task and Motion Planning for Imitation Learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:2310.16014</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Minaee, S., Mikolov, T., Nikzad, N., Chenaghlu, M., Socher, R., Amatriain, X., and Gao, J. (2025).

</span>
<span class="ltx_bibblock">Large Language Models: A Survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">arXiv preprint arXiv:2402.06196</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Morandín-Ahuerma, F. (2023).

</span>
<span class="ltx_bibblock">IEEE: a global standard as an ethical AI initiative.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">Proceedings of the International Conference on Artificial Intelligence Ethics</span>, pages 127–136.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Morley, J., Elhalal, A., Garcia, F., Balogh, A., Mökander, J., Kiron, D., Ziesche, M., Wacker, F., Hinds, R., Marinucci, A., Corazza, M., Taddeo, M., and Floridi, L. (2021).

</span>
<span class="ltx_bibblock">Ethics as a Service: A Pragmatic Operationalisation of AI Ethics.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">Minds and Machines</span>, 31.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Morris, M. R. (2020).

</span>
<span class="ltx_bibblock">AI and accessibility.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">Commun. ACM</span>, 63(6):35–37.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Pattnaik, N., Li, S., and Nurse, J. R. C. (2023).

</span>
<span class="ltx_bibblock">A Survey of User Perspectives on Security and Privacy in a Home Networking Environment.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">ACM Comput. Surv.</span>, 55(9):180:1–180:38.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Qi, X. and Yu, J. (2025).

</span>
<span class="ltx_bibblock">Participatory Design in Human-Computer Interaction: Cases, Characteristics, and Lessons.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems</span>, CHI ’25, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Ribeiro, M. T., Singh, S., and Guestrin, C. (2016).

</span>
<span class="ltx_bibblock">”Why Should I Trust You?”: Explaining the Predictions of Any Classifier.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:1602.04938</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Roberts, H., Cowls, J., Morley, J., Taddeo, M., Wang, V., and Floridi, L. (2021).

</span>
<span class="ltx_bibblock">The Chinese approach to artificial intelligence: an analysis of policy, ethics, and regulation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">AI &amp; SOCIETY</span>, 36, pages 59–77. 
</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.org/10.1007/s00146-020-00992-2</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Delgado, F., Yang, S., Madaio, M., and Yang, Q. (2023).

</span>
<span class="ltx_bibblock">The Participatory Turn in AI Design: Theoretical Foundations and the Current State of Practice.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">Proceedings of the 3rd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization</span>, Article No. 37.

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.org/10.1145/3617694.3623261</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Rudrapal, D., Jamatia, A., Chakma, K., Das, A., and Gambäck, B. (2015).

</span>
<span class="ltx_bibblock">Sentence Boundary Detection for Social Media Text.

</span>
<span class="ltx_bibblock">In Sharma, D. M., Sangal, R., and Sherly, E., editors, <span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">Proceedings of the 12th International Conference on Natural Language Processing</span>, pages 254–260, Trivandrum, India. NLP Association of India.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Sharkey, A. and Sharkey, N. (2010).

</span>
<span class="ltx_bibblock">Granny and the robots: Ethical issues in robot care for the elderly.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib40.1.1">Ethics and Information Technology</span>, 14:27–40.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Shen, Y., Song, K., Tan, X., Li, D., Lu, W., and Zhuang, Y. (2023).

</span>
<span class="ltx_bibblock">HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">arXiv preprint arXiv:2303.17580</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Shibata, T. and Coughlin, J. F. (2014).

</span>
<span class="ltx_bibblock">Trends of Robot Therapy with Neurological Therapeutic Seal Robot, PARO.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib42.1.1">Journal of Robotics and Mechatronics</span>, 26(4):418–425.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Yuan, S., Coghlan, S., Lederman, R., and Waycott, J. (2023).

</span>
<span class="ltx_bibblock">Ethical Design of Social Robots in Aged Care: A Literature Review Using an Ethics of Care Perspective.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib43.1.1">International Journal of Social Robotics</span>, 15, 1–18.

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.org/10.1007/s12369-023-01053-6</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Brown, T. B. and Mann, B. and Ryder, N. and Subbiah, M. and Kaplan, J. and Dhariwal, P. and Neelakantan, A. and Shyam, P. and Sastry, G. and Askell, A. and Agarwal, S. and Herbert-Voss, A. and Krueger, G. and Henighan, T. and Child, R. and Ramesh, A. and Ziegler, D. M. and Wu, J. and Winter, C. and Hesse, C. and Chen, M. and Sigler, E. and Litwin, M. and Gray, S. and Chess, B. and Clark, J. and Berner, C. and McCandlish, S. and Radford, A. and Sutskever, I. and Amodei, D. (2020).

</span>
<span class="ltx_bibblock">Language Models are Few-Shot Learners.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib44.1.1">arXiv preprint arXiv:2005.14165</span>.

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2005.14165</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Vilone, G. and Longo, L. (2021).

</span>
<span class="ltx_bibblock">Notions of explainability and evaluation approaches for explainable artificial intelligence.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib45.1.1">Information Fusion</span>, 76:89–106.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Voigt, P. and von dem Bussche, A. (2017).

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib46.1.1">The EU General Data Protection Regulation (GDPR): A Practical Guide</span>.

</span>
<span class="ltx_bibblock">Springer Cham, 1 edition.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Wang, L., Xu, W., Lan, Y., Hu, Z., Lan, Y., Lee, R. K.-W., and Lim, E.-P. (2023).

</span>
<span class="ltx_bibblock">Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib47.1.1">arXiv preprint arXiv:2305.04091</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Wang, Z., Ma, Y., Song, Y., Huang, Y., Liang, G., and Zhong, X. (2024).

</span>
<span class="ltx_bibblock">The Utilization of Natural Language Processing for Analyzing Social Media Data in Nursing Research: A Scoping Review.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib48.1.1">Journal of Nursing Management</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Wang, L., Anyi, C. L., Xu, K., Liu, Y., Arriaga, R. I., and Goel, A. K. (2025).

</span>
<span class="ltx_bibblock">Explainable AI for Daily Scenarios from End-Users’ Perspective: Non-Use, Concerns, and Ideal Design.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib49.1.1">Proceedings of the 2025 ACM Designing Interactive Systems Conference</span>, pages 2328–2349, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Weerts, H., Dudík, M., Edgar, R., Jalali, A., Lutz, R., and Madaio, M. (2023).

</span>
<span class="ltx_bibblock">Fairlearn: Assessing and Improving Fairness of AI Systems.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib50.1.1">arXiv preprint arXiv:2303.16626</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., Chi, E. H., Hashimoto, T., Vinyals, O., Liang, P., Dean, J., and Fedus, W. (2022).

</span>
<span class="ltx_bibblock">Emergent Abilities of Large Language Models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib51.1.1">arXiv preprint arXiv:2206.07682</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Werner, K., Kender, K., Scheepmaker, L., and Frauenberger, C. (2024).

</span>
<span class="ltx_bibblock">Technologies Supporting Social Play in Neurodiverse Groups of Children.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib52.1.1">Proceedings of the 23rd Annual ACM Interaction Design and Children Conference</span>, pages 218–231, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Wu, J., Gan, W., Chen, Z., Wan, S., and Yu, P. S. (2023).

</span>
<span class="ltx_bibblock">Multimodal Large Language Models: A Survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib53.1.1">arXiv preprint arXiv:2311.13165</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Yang, H., Yue, S., and He, Y. (2023).

</span>
<span class="ltx_bibblock">Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib54.1.1">arXiv preprint arXiv:2306.02224</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Alaa, M., Zaidan, A. A., Zaidan, B. B., Talal, M., and Kiah, M.L.M. (2017).

</span>
<span class="ltx_bibblock">A review of smart home applications based on Internet of Things.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib55.1.1">Journal of Network and Computer Applications</span>, 97, 48–65.

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.org/10.1016/j.jnca.2017.08.017</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Xu, W., Dainoff, M., Ge, L., and Gao, Z. (2021).

</span>
<span class="ltx_bibblock">Transitioning to human interaction with AI systems: New challenges and opportunities for HCI professionals to enable human-centered AI.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib56.1.1">arXiv preprint arXiv:2105.05424</span>. 
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Barredo Arrieta, A., Díaz-Rodríguez, N., Del Ser, J., Bennetot, A., Tabik, S., Barbado, A., Garcia, S., Gil-Lopez, S., Molina, D., Benjamins, R., Chatila, R., and Herrera, F. (2020).

</span>
<span class="ltx_bibblock">Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib57.1.1">Information Fusion</span>, 58, 82–115.

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.org/10.1016/j.inffus.2019.12.012</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Chandra, J., Algazinov, A., Navneet, S. K., El Filali, R., Laing, M., and Hanna, A. (2025).

</span>
<span class="ltx_bibblock">WebTrust: An AI-Driven Data Scoring System for Reliable Information Retrieval.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib58.1.1">arXiv preprint arXiv:2506.12072</span>.

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2506.12072</span>.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Jul 21 06:08:32 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
